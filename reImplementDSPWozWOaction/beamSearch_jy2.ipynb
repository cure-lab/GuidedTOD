{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline should be like:\n",
    "1. Core: could predict next action given the current context\n",
    "2. can load the data from the benchmark\n",
    "3. connect the data to the core module, to get the run-time action prediction list\n",
    "4. Use CE metric to evaluate the quality of the predicted action list\n",
    "5. if good enough, then save the dialogues as the new data for further training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only generate the action list, not the full dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_ast_prediction(prediction_str):\n",
    "    match = re.match(r\"(.*)\\[(.*)]\", prediction_str)\n",
    "    if match:\n",
    "        # action w/ value\n",
    "        action_name = match.group(1).strip()\n",
    "        slot_str = match.group(2)\n",
    "        slot_str = slot_str.replace(\";\", \",\")\n",
    "        slots = [s.strip() for s in slot_str.split(\",\")]\n",
    "        for i in range(len(slots)):\n",
    "            if slots[i].endswith(\">\") and not slots[i].startswith(\"<\"):\n",
    "                # add \"<\" to the beginning of the slot\n",
    "                slots[i] = \"<\" + slots[i]\n",
    "            if slots[i].startswith(\"<\") and not slots[i].endswith(\">\"):\n",
    "                # add \">\" to the end of the slot\n",
    "                slots[i] = slots[i] + \">\"\n",
    "    else:\n",
    "        action_name = \"MISSING\"\n",
    "        slots = [\"MISSING\"]\n",
    "\n",
    "    return action_name, slots\n",
    "\n",
    "\n",
    "def parse_context_action(context):\n",
    "    # Regular expression pattern to match actions (XX-XX, XX-XX-XX or XX-XX-XX-XX)\n",
    "    action_pattern = r\"\\b(\\w+-\\w+(?:-\\w+){0,2})(?=\\s*\\[)\"\n",
    "\n",
    "    # Find all matches in the context string\n",
    "    actions = re.findall(action_pattern, context)\n",
    "    return actions\n",
    "\n",
    "def normalize_log_probs(log_probs):\n",
    "    \"\"\"\n",
    "    Normalizes an array of log probabilities using the Log-Sum-Exp trick\n",
    "    to prevent numerical underflow.\n",
    "\n",
    "    Parameters:\n",
    "    - log_probs: A numpy array containing log probabilities.\n",
    "\n",
    "    Returns:\n",
    "    - An array of normalized log probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the maximum log probability to avoid underflow issues\n",
    "    max_log_prob = np.max(log_probs)\n",
    "    \n",
    "    # Subtract the max log probability and exponentiate the result\n",
    "    probs_stable = np.exp(log_probs - max_log_prob) + 1e-9\n",
    "    \n",
    "    # Sum the stabilized probabilities\n",
    "    prob_total_stable = np.sum(probs_stable) + 1e-9\n",
    "    \n",
    "    # Normalize the stabilized probabilities\n",
    "    normalized_probs_stable = probs_stable / prob_total_stable\n",
    "    \n",
    "    # Convert back to log probabilities adding the subtracted max_log_prob\n",
    "    normalized_log_probs_stable = np.log(normalized_probs_stable) + max_log_prob\n",
    "    \n",
    "    return normalized_log_probs_stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/d1/gds/jyzhong/miniconda3/envs/comp-model/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.metrics import *\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "data_args = SimpleNamespace(\n",
    "    test_file='/research/d1/gds/jyzhong/computation_models/LLMFramework/data/processed/test_AST_abcd_waction_full.json',\n",
    "    max_predict_samples=1000,\n",
    ")\n",
    "\n",
    "training_args = SimpleNamespace(\n",
    "    use_bert_score=False,\n",
    "    use_ast_metrics=True\n",
    ")\n",
    "\n",
    "\n",
    "def create_compute_metric_fct( data_args, training_args):\n",
    "    def decode(preds, labels):\n",
    "        # if isinstance(preds, tuple):\n",
    "        #     preds = preds[0]\n",
    "        # if data_args.ignore_pad_token_for_loss:\n",
    "        #     # Replace -100 in the labels as we can't decode them.\n",
    "        #     preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        # decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        # if data_args.ignore_pad_token_for_loss:\n",
    "        #     # Replace -100 in the labels as we can't decode them.\n",
    "        #     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        # decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(preds, labels)\n",
    "\n",
    "        # model_path = Path(model_args.model_name_or_path)\n",
    "        # file_name = \"pred_mwoz.txt\" if training_args.is_mwoz else \"preds_test_set.txt\"\n",
    "        # if not model_path.exists():\n",
    "        #     # model name\n",
    "        #     preds_file_path = Path(training_args.output_dir) / file_name\n",
    "        # else:\n",
    "        #     preds_file_path = model_path / file_name\n",
    "\n",
    "        # with preds_file_path.open(\"w\") as f:\n",
    "        #     for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        #         label = label.replace(\"\\n\", \" \")\n",
    "        #         pred = pred.replace(\"\\n\", \" \")\n",
    "        #         f.write(f\"{pred}\\t{label}\" + \"\\n\")\n",
    "\n",
    "        return decoded_preds, decoded_labels\n",
    "\n",
    "    def parse_predictions(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        decoded_predictions, decoded_labels = decode(preds, labels)\n",
    "        return decoded_predictions, decoded_labels\n",
    "\n",
    "    def compute_em_and_ce(eval_preds):\n",
    "        predictions, labels = parse_predictions(eval_preds)\n",
    "        predictions = [parse_workflow_string(w) for w in predictions]\n",
    "        labels = [parse_workflow_string(w) for w in labels]\n",
    "        return compute_metrics(labels, predictions, use_bert_score=training_args.use_bert_score)\n",
    "\n",
    "    def compute_cds_metrics(eval_preds):\n",
    "        predictions, labels = parse_predictions(eval_preds)\n",
    "        # print(\"data_args.test_file\", data_args.test_file)\n",
    "        # print(\"data_args.max_predict_samples\", data_args.max_predict_samples)\n",
    "        convo_ids, turn_ids = load_raw_test_dataset(data_args.test_file, data_args.max_predict_samples)\n",
    "        return compute_cds_em_and_ce(predictions, labels, convo_ids, turn_ids)\n",
    "    \n",
    "    def compute_ast_metrics(eval_preds, sequence_scores=None):\n",
    "        predictions, labels = parse_predictions(eval_preds)\n",
    "        is_eval = True if len(labels) == 3684 else False\n",
    "        conv_ids, turn_ids = load_raw_test_dataset(data_args.test_file, data_args.max_predict_samples)\n",
    "        print(\"predictions:\", len(predictions))\n",
    "        print(\"labels:\", len(labels))\n",
    "        print(\"conv_ids:\", len(conv_ids))\n",
    "        print(\"turn_ids:\", len(turn_ids))\n",
    "        # print(\"sequence_scores:\", len(sequence_scores))\n",
    "        '''\n",
    "        predictions: 200\n",
    "        labels: 50\n",
    "        conv_ids: 50\n",
    "        turn_ids: 50\n",
    "        sequence_scores: (200,)\n",
    "        '''\n",
    "        return compute_ast_acc_metrics(predictions, labels, conv_ids, turn_ids, sequence_scores)\n",
    "\n",
    "    def no_metrics(eval_preds):\n",
    "        # Evaluation will be done during post hf_training\n",
    "        preds, labels = eval_preds\n",
    "        decode(preds, labels)\n",
    "        return {}\n",
    "\n",
    "    return compute_ast_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### directly load the model (OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load model directly\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/research/d1/gds/jyzhong/computation_models/LLMFramework/results/abcdASTWActionAll_input_target_t5-small/checkpoint-45700\",\n",
    "    use_fast=False,\n",
    "    # revision=\"main\",\n",
    "    # use_auth_token=None,\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"/research/d1/gds/jyzhong/computation_models/LLMFramework/results/abcdASTWActionAll_input_target_t5-small/checkpoint-45700\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.is_encoder_decoder = True\n",
    "\n",
    "data = []\n",
    "with open('/research/d1/gds/jyzhong/computation_models/LLMFramework/data/processed/test_AST_abcd_waction_full.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_data = json.loads(line)\n",
    "        data.append(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/research/d1/gds/jyzhong/miniconda3/envs/comp-model/lib/python3.11/site-packages/transformers/generation/utils.py:1256: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "  5%|▍         | 48/1000 [00:33<09:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute 47, set prob to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 305/1000 [02:52<05:05,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute 304, set prob to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 952/1000 [08:30<00:17,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute 951, set prob to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:54<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pull-up-account [chloe zhang]', 'search-timing [none]', 'pull-up-account [albert sanders], account_id>, account_id>]', 'verify-identity [albert sanders, account_id>, account_id>] \"account_id>]', \"enter-details [account_id>], account_id>, account_id>] 'i've been charged twice for it. how can i help?silver?silver?silver?silver?silver?silver?silver?silver?silver?soulver?soulver?soulver?\"]\n",
      "['search-faq [none]', 'search-timing [none]', 'pull-up-account [albert sanders]', 'verify-identity [albert sanders, <account_id>, <account_id>]', 'membership [silver]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from aalpy.utils import load_automaton_from_file\n",
    "\n",
    "# load an automaton\n",
    "chained_proir = load_automaton_from_file('/research/d1/gds/jyzhong/computation_models/mdp_guildline_half.dot', automaton_type='mc')\n",
    "\n",
    "pred_pm = []\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "# for i in tqdm(range(len(data))):\n",
    "for i in tqdm(range(1000)):\n",
    "    # print('=' * 100)\n",
    "    # print(\"input: \", data[i]['input'])\n",
    "    # print(\"target: \", data[i]['target'])\n",
    "    labels.append(data[i]['target'])\n",
    "\n",
    "    input_context = \"Predict AST: \" + data[i]['input']\n",
    "    encoder_input_ids = tokenizer(input_context, return_tensors=\"pt\")\n",
    "    encoder_input_ids = encoder_input_ids.to(device)\n",
    "\n",
    "    # lets run beam search using 3 beams\n",
    "    num_beams = 35\n",
    "    # define decoder start token ids\n",
    "    outputs = model.generate(**encoder_input_ids, max_new_tokens=100, return_dict_in_generate=True, output_scores=True, do_sample=False, num_beams=num_beams, num_return_sequences=num_beams,)\n",
    "    # print(outputs.scores)\n",
    "    try:\n",
    "        transition_scores = model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=False\n",
    "        )\n",
    "        output_length = np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
    "        length_penalty = model.generation_config.length_penalty\n",
    "        reconstructed_scores = transition_scores.sum(axis=1).cpu().numpy() / (output_length**length_penalty)\n",
    "    except:\n",
    "        reconstructed_scores = np.zeros(num_beams)\n",
    "        print(f\"cannot compute {i}, set prob to 0\")\n",
    "\n",
    "    log_prob = reconstructed_scores\n",
    "    # log_prob = np.sum(log_prob, axis=1)\n",
    "    # log_prob = normalize_log_probs(log_prob)\n",
    "    # print(f'scores: {log_prob}')\n",
    "\n",
    "    policy_model_action = []\n",
    "    policy_model_slot = []\n",
    "    policy_model_prob = []\n",
    "    policy_max_idx = np.argmax(log_prob)\n",
    "\n",
    "    # print('+' * 20, 'policy model', '+' * 20)\n",
    "    for k in range(num_beams):\n",
    "        pred_str = tokenizer.decode(outputs.sequences[k], skip_special_tokens=True)\n",
    "        action_name, slots = parse_ast_prediction(pred_str)\n",
    "\n",
    "        policy_model_action.append(action_name)\n",
    "        policy_model_slot.append(slots)\n",
    "        if action_name != 'MISSING':\n",
    "            policy_model_prob.append(log_prob[k])\n",
    "        else:\n",
    "            policy_model_prob.append(-1000)\n",
    "\n",
    "        if k == policy_max_idx:\n",
    "            pred_pm.append(tokenizer.decode(outputs.sequences[policy_max_idx], skip_special_tokens=True))\n",
    "            # print(\"policy model top pred str: \", tokenizer.decode(outputs.sequences[policy_max_idx], skip_special_tokens=True))\n",
    "            # print(f\"policy model top parsed: {action_name}, {slots}; score: {log_prob[policy_max_idx]}\")\n",
    "\n",
    "\n",
    "    # print('+' * 20, 'chained prior', '+' * 20)\n",
    "    prev_actions = parse_context_action(input_context)\n",
    "    prev_actions = ['init'] + prev_actions\n",
    "\n",
    "    init_state = chained_proir.states[0]\n",
    "    curr_state = init_state\n",
    "\n",
    "    prev_actions_with_prob = [(curr_state.output, 1)]\n",
    "    prev_prob = 0\n",
    "    idx = 0\n",
    "    while curr_state.output != prev_actions[-1] and idx <= len(prev_actions) - 1:\n",
    "        \n",
    "        idx += 1\n",
    "        matched = False\n",
    "        for prosible_state, prob in curr_state.transitions:\n",
    "            if prosible_state.output == prev_actions[idx]:\n",
    "                prev_actions_with_prob.append((prosible_state.output, np.log(prob)))\n",
    "                prev_prob += np.log(prob)\n",
    "                curr_state = prosible_state\n",
    "                matched = True\n",
    "        \n",
    "        if matched == False:\n",
    "            break\n",
    "    # print(f\"context parsed prob: {prev_actions_with_prob}, total previous prob: {prev_prob}\")\n",
    "    \n",
    "    chained_prior_pred = []\n",
    "    chained_prior_prob = []\n",
    "    for state in chained_proir.states:\n",
    "        if state.output == prev_actions[-1]:\n",
    "            for prosible_state, prob in state.transitions:\n",
    "                chained_prior_pred.append(prosible_state.output)\n",
    "                chained_prior_prob.append(np.log(prob) + prev_prob)\n",
    "    chained_prior_prob = normalize_log_probs(chained_prior_prob) * 5\n",
    "    # print(f\"possible next: {chained_prior_pred}, prob: {chained_prior_prob}\")\n",
    "\n",
    "\n",
    "    alpha = 0.6\n",
    "    for i, action in enumerate(policy_model_action):\n",
    "        for j, c_action in enumerate(chained_prior_pred):\n",
    "            if action == c_action:\n",
    "                policy_model_prob[i] = policy_model_prob[i] * (1 - alpha) + chained_prior_prob[j] * alpha\n",
    "\n",
    "    # print('+' * 20, 'chained prior + policy model', '+' * 20)\n",
    "    top_idx = np.argmax(np.array(policy_model_prob))\n",
    "    # print(\"MERGED top pred str: \", f\"\")\n",
    "    # print(f\"MERGED top parsed: {policy_model_action[top_idx]}, {policy_model_slot[top_idx]}; score: {policy_model_prob[top_idx]}\")\n",
    "\n",
    "    preds.append(tokenizer.decode(outputs.sequences[top_idx], skip_special_tokens=True))\n",
    "\n",
    "print(preds[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chained + PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: 1000\n",
      "labels: 1000\n",
      "conv_ids: 1000\n",
      "turn_ids: 1000\n",
      "['search-faq [none]', 'search-timing [none]', 'pull-up-account [albert sanders]', 'verify-identity [albert sanders, <account_id>, <account_id>]', 'membership [silver]', 'ask-the-oracle [none]', 'search-faq [none]', 'search-shirt [none]', 'search-faq [none]', 'search-policy [none]']\n",
      "['pull-up-account [chloe zhang]', 'search-timing [none]', 'pull-up-account [albert sanders], account_id>, account_id>]', 'verify-identity [albert sanders, account_id>, account_id>] \"account_id>]', \"enter-details [account_id>], account_id>, account_id>] 'i've been charged twice for it.\\nhow can i help?silver?silver?silver?silver?silver?silver?silver?silver?silver?soulver?soulver?soulver?\", 'offer-refund [20) - see if account_id> account_id> account_id> account_id> account_id> account_id> account_id> account_id> account_id>]', 'search-faq [none]', 'purchase-tommy hilifiger shirt [tommy hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger.\\nsearch-shirt [none]', \"search-faq [none]:how can i help you?how can i help you?i'm sorry that you might want to cancel your subscription.\\ndid something happen that made you want to cancel your subscription?none, i'm just thinking of ordering some things and don't want to do this.\\nlet me know if the process is too hard.\\nlet me know what i can find for you.\", 'select-faq [policy_2]']\n"
     ]
    }
   ],
   "source": [
    "metric = create_compute_metric_fct(data_args, training_args)\n",
    "results = metric((preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EM_action': 0.2143, 'EM_value': 0.1071, 'EM_joint': 0.0786, 'turn_acc_joint': 0.34, 'turn_acc_action': 0.569, 'turn_acc_value': 0.407, 'CE_joint': 0.241, 'CE_action': 0.407, 'CE_value': 0.2962}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: 1000\n",
      "labels: 1000\n",
      "conv_ids: 1000\n",
      "turn_ids: 1000\n",
      "['search-faq [none]', 'search-timing [none]', 'pull-up-account [albert sanders]', 'verify-identity [albert sanders, <account_id>, <account_id>]', 'membership [silver]', 'ask-the-oracle [none]', 'search-faq [none]', 'search-shirt [none]', 'search-faq [none]', 'search-policy [none]']\n",
      "['verify-identity [chloe zhang, michael kors kors kors kors kors kors kors kors kors kors kors kors kors korkors kors kors kors kors kors kors kors kors kors kors kors kors kors', 'verify-identity [chloe zhang, search-timingsnormanuelnuelnuelnuelnuelnuelnuelnueltuelnuelnuelnuelnuelnuelnuelnuelnuelnuelnuelnuel', 'verify-identity [albert sanders, account_id>, account_id>]: \"i recently signed up for a subscription but it seems like you guys charged me twice for it.\\nlet\\'s fix that.\\nlet\\'s try to fix that.\\nmay i have your full name and order id?i have your full account id: account_id>]', 'verify-identity [albert sanders, account_id>, account_id>] \"account_id>]', \"enter-details [account_id>], account_id>, account_id>] 'i've been charged twice for it.\\nhow can i help?silver?silver?silver?silver?silver?silver?silver?silver?silver?soulver?soulver?soulver?\", \"offer-refund [20) - see if account_id> 'account_id> 'account_id> 'account_id> 'account_id> 'account_id> 'account_id> 'account_id> 'account_id> 'account_i\", \"verify-identity [hi!how can i help you?i'm thinking about buying an item but i'd like to know more about it, then i'd like to know more about it?i would like to know how long is the arm length sure give me one second and i can find that out for you.\\nwhat is the tommy hilifiger shirt and what would you like to know about it?i would like to know\", 'purchase-tommy hilifiger shirt [tommy hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger!\\nhilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger hilfiger', \"search-faq [none]:how can i help you?how can i help you?i'm sorry that you might want to cancel your subscription.\\ndid something happen that made you want to cancel your subscription?none, i'm just thinking of ordering some things and don't want to do this.\\nlet me know if the process is too hard.\\nlet me know what i can find for you.\", 'select-faq [policy_2]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EM_action': 0.0036,\n",
       " 'EM_value': 0.0321,\n",
       " 'EM_joint': 0.0036,\n",
       " 'turn_acc_joint': 0.107,\n",
       " 'turn_acc_action': 0.229,\n",
       " 'turn_acc_value': 0.241,\n",
       " 'CE_joint': 0.0722,\n",
       " 'CE_action': 0.1361,\n",
       " 'CE_value': 0.1357}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = create_compute_metric_fct(data_args, training_args)\n",
    "results = metric((pred_pm, labels))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workflowExtract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
