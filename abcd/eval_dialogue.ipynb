{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline should be like:\n",
    "1. Core: could predict next action given the current context\n",
    "2. can load the data from the benchmark\n",
    "3. connect the data to the core module, to get the run-time action prediction list\n",
    "4. Use CE metric to evaluate the quality of the predicted action list\n",
    "5. if good enough, then save the dialogues as the new data for further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set the openai GPT-4\n",
    "'''\n",
    "from openai import OpenAI\n",
    "clientGPT4 = OpenAI(api_key=\"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\")\n",
    "clientGPT3_5 = OpenAI(api_key=\"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\")\n",
    "\n",
    "'''\n",
    "Set the AST module for predict the next action: \n",
    "For demo, use the model for SGD dataset \n",
    "'''\n",
    "\"\"\"\n",
    "Reference: https://github.com/huggingface/transformers/tree/main/examples/pytorch\n",
    "\n",
    "Adapted from huggingface Transformers\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "import transformers.trainer_utils as hf_trainer_utils\n",
    "import numpy as np\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    ")\n",
    "\n",
    "from src.data.data_args import DataArguments\n",
    "from src.data.dataset_loader import DatasetLoader\n",
    "from src.data.utils import group_col_name\n",
    "from src.metrics import create_compute_metric_fct, verify_nltk\n",
    "from src.model.hf_model_args import HfModelArguments\n",
    "from src.hf_training.hf_training_args import HfSeq2SeqTrainingArgs\n",
    "# set cuda\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def train(trainer, train_dataset, training_args):\n",
    "    logger.info(\"*** train ***\")\n",
    "\n",
    "    check_point = get_resume_checkpoint(training_args)\n",
    "    train_result = trainer.train(resume_from_checkpoint=check_point)\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"train_samples\"] = len(train_dataset)\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "\n",
    "def do_eval(trainer, validation_dataset, max_length, num_beams):\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n",
    "\n",
    "    metrics[\"eval_samples\"] = len(validation_dataset)\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "def do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams):\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "        return preds, labels\n",
    "\n",
    "    def decode(preds, labels):\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        if data_args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        if data_args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        model_path = Path(model_args.model_name_or_path)\n",
    "        file_name = \"pred_mwoz.txt\" if training_args.is_mwoz else \"preds_test_set.txt\"\n",
    "        if not model_path.exists():\n",
    "            # model name\n",
    "            preds_file_path = Path(training_args.output_dir) / file_name\n",
    "        else:\n",
    "            preds_file_path = model_path / file_name\n",
    "\n",
    "        with preds_file_path.open(\"w\") as f:\n",
    "            for pred, label in zip(decoded_preds, decoded_labels):\n",
    "                label = label.replace(\"\\n\", \" \")\n",
    "                pred = pred.replace(\"\\n\", \" \")\n",
    "                f.write(f\"{pred}\\t{label}\" + \"\\n\")\n",
    "\n",
    "        return decoded_preds, decoded_labels\n",
    "    \n",
    "    logger.info(\"*** Predict ***\")\n",
    "\n",
    "    metrics = {}\n",
    "    predictions = []\n",
    "    if group_col_name in test_dataset.column_names:\n",
    "        group_idx = 0\n",
    "\n",
    "        while True:\n",
    "            group_dataset = test_dataset.filter(lambda x: x[group_col_name] == group_idx)\n",
    "            if group_dataset.num_rows == 0:\n",
    "                # no groups left\n",
    "                break\n",
    "            logger.info(\"Predicting on test group %d\", group_idx)\n",
    "\n",
    "            predict_results = trainer.predict(\n",
    "                group_dataset,\n",
    "                metric_key_prefix=f\"predict_group_{group_idx}\",\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams\n",
    "            )\n",
    "            metrics.update(predict_results.metrics)\n",
    "            metrics[f\"predict_samples_group_{group_idx}_size\"] = len(group_dataset)\n",
    "\n",
    "            group_idx += 1\n",
    "\n",
    "            predictions.append(predict_results.predictions)\n",
    "\n",
    "        for key in [\"loss\", \"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "            metrics[f\"overall_predict_{key}\"] = round(\n",
    "                sum([metrics[f\"predict_group_{idx}_{key}\"] for idx in range(group_idx)]) / group_idx, 4\n",
    "            )\n",
    "    else:\n",
    "        '''\n",
    "        here\n",
    "        '''\n",
    "        # print(\"test_dataset.column_names: \", test_dataset.column_names)\n",
    "        # print(\"test_dataset: \", test_dataset)\n",
    "        # print(\"test_dataset[:2]: \", test_dataset[:2])\n",
    "        # sample_test_dataset = test_dataset.filter(lambda x: x[\"sample_id\"] in [0, 1, 3])\n",
    "        # print(\"sample_test_dataset[\\\"sample_id\\\"]: \", sample_test_dataset[\"sample_id\"])\n",
    "        # sample_test_dataset[\"sample_id\"] = [0, 1, 2]\n",
    "        # print(\"sample_test_dataset[\\\"sample_id\\\"]: \", sample_test_dataset[\"sample_id\"])\n",
    "        # sample_test_dataset[\"input_ids\"] = [sample_test_dataset[\"input_ids\"][0], sample_test_dataset[\"input_ids\"][1], [1,2,3,4,5]]\n",
    "        # print(\"sample_test_dataset[\\\"input_ids\\\"]: \", sample_test_dataset[\"input_ids\"])\n",
    "\n",
    "        # print(\"sample_test_dataset: \", sample_test_dataset)\n",
    "        # print(test_dataset[\"sample_id\"])\n",
    "        # print(test_dataset[\"input_ids\"])\n",
    "        # print(test_dataset[\"labels\"])\n",
    "        \n",
    "        predict_results = trainer.predict(\n",
    "            test_dataset, \n",
    "            metric_key_prefix=\"test\", \n",
    "            max_length=max_length, \n",
    "            num_beams=num_beams,\n",
    "            return_dict_in_generate=True, \n",
    "            num_return_sequences=num_beams,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        print(\"predict_results: \", predict_results)\n",
    "        # print(\"predict_results.predictions: \", predict_results.predictions)\n",
    "        metrics = predict_results.metrics\n",
    "        metrics[\"predict_samples_size\"] = len(test_dataset)\n",
    "\n",
    "    # trainer.log(metrics)\n",
    "    # trainer.log_metrics(\"test\", metrics)\n",
    "    # trainer.save_metrics(\"test\", metrics)\n",
    "\n",
    "    return decode(predict_results.predictions, test_dataset[\"labels\"])\n",
    "\n",
    "\n",
    "def load_model(model_args, data_args, tokenizer):\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    # Forcing the generation min lenght, to avoid models preset for summarization tasks that are usually high\n",
    "    config.min_length = 5\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    if task_specific_params is not None:\n",
    "        model.config.update(task_specific_params.get(\"summarization_cnn\", {}))\n",
    "\n",
    "    if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n",
    "        if isinstance(tokenizer, MBartTokenizer):\n",
    "            model.config.decoder_start_token_id = tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "        else:\n",
    "            model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(\"en_XX\")\n",
    "\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "    if (\n",
    "        hasattr(model.config, \"max_position_embeddings\")\n",
    "        and model.config.max_position_embeddings < data_args.max_source_length\n",
    "    ):\n",
    "        if model_args.resize_position_embeddings is None:\n",
    "            logger.warning(\n",
    "                \"Increasing the model's number of position embedding vectors from\"\n",
    "                f\" {model.config.max_position_embeddings} to {data_args.max_source_length}.\"\n",
    "            )\n",
    "            model.resize_position_embeddings(data_args.max_source_length)\n",
    "        elif model_args.resize_position_embeddings:\n",
    "            model.resize_position_embeddings(data_args.max_source_length)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`--max_source_length` is set to {data_args.max_source_length}, but the model only has\"\n",
    "                f\" {model.config.max_position_embeddings} position encodings. Consider either reducing\"\n",
    "                f\" `--max_source_length` to {model.config.max_position_embeddings} or to automatically resize the\"\n",
    "                \" model's position encodings by passing `--resize_position_embeddings`.\"\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_resume_checkpoint(training_args):\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    last_checkpoint = get_last_checkpoint(training_args)\n",
    "    if last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def get_last_checkpoint(training_args):\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = hf_trainer_utils.get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming hf_training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "    return last_checkpoint\n",
    "\n",
    "\n",
    "def setup_logging(training_args):\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "def create_data_collector(model, tokenizer, training_args, data_args):\n",
    "    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    return DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "\n",
    "\n",
    "def setup_wandb(training_args):\n",
    "    if training_args.use_wandb:\n",
    "        os.environ[\"WANDB_PROJECT\"] = training_args.wandb_project_name\n",
    "        training_args.run_name = training_args.experiment_name\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = HfArgumentParser((HfModelArguments, DataArguments, HfSeq2SeqTrainingArgs))\n",
    "    model_args, data_args, training_args, _ = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "\n",
    "    name_parts = [training_args.experiment_name]\n",
    "    name_parts.extend([data_args.text_column, data_args.summary_column])\n",
    "\n",
    "    name_parts.append(model_args.model_name_or_path)\n",
    "\n",
    "    training_args.experiment_name = \"_\".join(name_parts)\n",
    "\n",
    "    training_args.output_dir = str(Path(training_args.output_dir).joinpath(training_args.experiment_name))\n",
    "\n",
    "    if data_args.source_prefix is None and model_args.model_name_or_path in [\n",
    "        \"t5-small\",\n",
    "        \"t5-base\",\n",
    "        \"t5-large\",\n",
    "        \"t5-3b\",\n",
    "        \"t5-11b\",\n",
    "    ]:\n",
    "        logger.warning(\n",
    "            \"You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with \"\n",
    "            \"`--source_prefix 'summarize: ' `\"\n",
    "        )\n",
    "    return data_args, model_args, training_args\n",
    "\n",
    "# def hf_run():\n",
    "data_args, model_args, training_args = get_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "setup_wandb(training_args)\n",
    "\n",
    "setup_logging(training_args)\n",
    "\n",
    "verify_nltk()\n",
    "\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: % distributed hf_training: %s 16-bits hf_training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "datasets_loader = DatasetLoader(data_args, training_args, tokenizer)\n",
    "train_dataset, validation_dataset, test_dataset = datasets_loader.load_datasets()\n",
    "\n",
    "model = load_model(model_args, data_args, tokenizer)\n",
    "\n",
    "if training_args.label_smoothing_factor > 0 and not hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "    logger.warning(\n",
    "        \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n",
    "        \"`%s`. This will lead to loss being calculated twice and will take up more memory\",\n",
    "        model.__class__.__name__,\n",
    "    )\n",
    "metric_fct = create_compute_metric_fct(tokenizer, data_args, training_args, model_args)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=create_data_collector(model, tokenizer, training_args, data_args),\n",
    "    compute_metrics=metric_fct if training_args.predict_with_generate else None,\n",
    ")\n",
    "\n",
    "if training_args.do_train:\n",
    "    train(trainer, train_dataset, training_args)\n",
    "\n",
    "max_length = (\n",
    "    training_args.generation_max_length\n",
    "    if training_args.generation_max_length is not None\n",
    "    else data_args.val_max_target_length\n",
    ")\n",
    "num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n",
    "# if training_args.do_eval:\n",
    "#     do_eval(trainer, validation_dataset, max_length, num_beams)\n",
    "\n",
    "# if training_args.do_predict:\n",
    "#     results_pred, results_label = do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams)\n",
    "    # print(\"results_pred: \", results_pred)\n",
    "    # print(\"results_label: \", results_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def postprocess_predictions(prediction_str):\n",
    "    # print(\"prediction_str: \", prediction_str)\n",
    "    match = re.match(r\"(.*)\\[(.*)]\", prediction_str)\n",
    "    if match:\n",
    "        # action w/ value\n",
    "        action_name = match.group(1).strip()\n",
    "        slot_str = match.group(2)\n",
    "        slot_str = slot_str.replace(\";\", \",\")\n",
    "        slots = [s.strip() for s in slot_str.split(\",\")]\n",
    "        for i in range(len(slots)):\n",
    "            if slots[i].endswith(\">\") and not slots[i].startswith(\"<\"):\n",
    "                # add \"<\" to the beginning of the slot\n",
    "                slots[i] = \"<\" + slots[i]\n",
    "            if slots[i].startswith(\"<\") and not slots[i].endswith(\">\"):\n",
    "                # add \">\" to the end of the slot\n",
    "                slots[i] = slots[i] + \">\"\n",
    "        post_str = action_name + \" \" + \"[\" + \", \".join(slots) + \"]\"\n",
    "        # print(\"post_str: \", post_str)\n",
    "        return post_str\n",
    "    else:\n",
    "        return prediction_str\n",
    "\n",
    "def parse_ast_prediction(prediction_str):\n",
    "    match = re.match(r\"(.*)\\[(.*)]\", prediction_str)\n",
    "    if match:\n",
    "        # action w/ value\n",
    "        action_name = match.group(1).strip()\n",
    "        slot_str = match.group(2)\n",
    "        slot_str = slot_str.replace(\";\", \",\")\n",
    "        slots = [s.strip() for s in slot_str.split(\",\")]\n",
    "        for i in range(len(slots)):\n",
    "            if slots[i].endswith(\">\") and not slots[i].startswith(\"<\"):\n",
    "                # add \"<\" to the beginning of the slot\n",
    "                slots[i] = \"<\" + slots[i]\n",
    "            if slots[i].startswith(\"<\") and not slots[i].endswith(\">\"):\n",
    "                # add \">\" to the end of the slot\n",
    "                slots[i] = slots[i] + \">\"\n",
    "    else:\n",
    "        action_name = \"MISSING\"\n",
    "        slots = [\"MISSING\"]\n",
    "\n",
    "    return action_name, slots\n",
    "\n",
    "def compute_ast_acc_metrics(predictions, labels, convo_ids, turn_ids, sequence_scores=None, num_beams=None):\n",
    "    # print(\"len(predictions): \", len(predictions))\n",
    "    # print(\"len(labels): \", len(labels))\n",
    "    # print(\"len(sequence_scores): \", len(sequence_scores))\n",
    "    from aalpy.utils import load_automaton_from_file\n",
    "\n",
    "    # load an automaton\n",
    "    automaton = load_automaton_from_file(\"./chainPrior/learned_mdp_8000.dot\", automaton_type='mdp')\n",
    "    # print(automaton)\n",
    "    # visualize the automaton\n",
    "    # visualize_automaton(automaton)\n",
    "    automaton = str(automaton)\n",
    "    # print(automaton)\n",
    "\n",
    "    automaton_splits = automaton.split('\\n')\n",
    "    # print(automaton_splits)\n",
    "    automaton_states = automaton_splits[1:33]\n",
    "    # ['s0 [label=\"init\"];', 's1 [label=\"pull-up-account\"];']\n",
    "    automaton_transitions = automaton_splits[33:-4]\n",
    "    # ['s0 -> s0  [label=\"init:1.0\"];', 's0 -> s1  [label=\"action:0.03\"];']\n",
    "\n",
    "    state_mapping = {}\n",
    "    for state in automaton_states:\n",
    "        state_name = state.split(' ')[0]\n",
    "        state_label = state.split('[label=\"')[1].split('\"];')[0]\n",
    "        state_mapping[state_name] = state_label\n",
    "\n",
    "    '''\n",
    "    state_mapping: {'s0': 'init', 's1': 'pull-up-account', 's2': 'enter-details', 's3': 'verify-identity', 's4': 'make-password', 's5': 'search-timing', 's6': 'search-policy', 's7': 'validate-purchase', 's8': 'search-faq', 's9': 'membership', 's10': 'search-boots', 's11': 'try-again', 's12': 'ask-the-oracle', 's13': 'update-order', 's14': 'promo-code', 's15': 'update-account', 's16': 'search-membership', 's17': 'make-purchase', 's18': 'offer-refund', 's19': 'notify-team', 's20': 'record-reason', 's21': 'search-jeans', 's22': 'shipping-status', 's23': 'search-shirt', 's24': 'instructions', 's25': 'search-jacket', 's26': 'log-out-in', 's27': 'select-faq', 's28': 'subscription-status', 's29': 'send-link', 's30': 'search-pricing', 's31': 'end'}\n",
    "    '''\n",
    "    # print(f\"state_mapping: {state_mapping}\")\n",
    "\n",
    "    transition_mapping = {}\n",
    "    for transition in automaton_transitions:\n",
    "        transition_split = transition.split('->')\n",
    "        source_state = transition_split[0].strip()\n",
    "        target_state = transition_split[1].strip().split(' ')[0]\n",
    "        transition_label = transition_split[1].split('[label=\"')[1].split('\"];')[0]\n",
    "        transition_action = transition_label.split(':')[0]\n",
    "        transition_freq = np.log(float(transition_label.split(':')[1])) if float(transition_label.split(':')[1]) > 0 else -10000\n",
    "        transition_mapping[(state_mapping[source_state], state_mapping[target_state])] = (transition_action, transition_freq)\n",
    "\n",
    "    # from s0 to s31, if some pair of states are not in the transition_mapping, then the frequency is 0\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            if (state_mapping[f's{i}'], state_mapping[f's{j}']) not in transition_mapping:\n",
    "                transition_mapping[(state_mapping[f's{i}'], state_mapping[f's{j}'])] = ('unknown', -10000)\n",
    "\n",
    "    # group predictions every 4\n",
    "    new_predictions = []\n",
    "    for i in range(0, len(predictions), num_beams):\n",
    "        new_predictions.append(predictions[i:i+num_beams])\n",
    "        # new_predictions.append(predictions[i])\n",
    "    \n",
    "    new_sequence_scores = []\n",
    "    for i in range(0, len(sequence_scores), num_beams):\n",
    "        # print(f\"sequence_scores[i:i+4]: {sequence_scores[i:i+4]}\")\n",
    "        # do no use normalization\n",
    "        new_sequence_scores.append(sequence_scores[i:i+num_beams])\n",
    "\n",
    "    \n",
    "    previous_actions = ['init']\n",
    "    current_convo_id = 999999\n",
    "    new_new_predictions = []\n",
    "    not_in_counter = 0\n",
    "    for new_pred, label1, new_sequence_score, convo_id1, turn_id1 in zip(new_predictions, labels, new_sequence_scores, convo_ids, turn_ids):\n",
    "        if convo_id1 != current_convo_id:\n",
    "            previous_actions = ['init']\n",
    "            current_convo_id = convo_id1\n",
    "        \n",
    "        actions = []\n",
    "        for pred in new_pred:\n",
    "            actions.append(pred.split(' ')[0].strip())\n",
    "\n",
    "        rates = []\n",
    "        for i in range(len(actions)):\n",
    "            try:\n",
    "                rate = transition_mapping[(previous_actions[-1], actions[i])][1]\n",
    "            except:\n",
    "                rate = -10000\n",
    "            rates.append(rate)\n",
    "        rates = np.array(rates)\n",
    "\n",
    "        '''\n",
    "        the way to merge the two modules for post processng, v1\n",
    "        '''\n",
    "        # print(f\"new_sequence_score: {new_sequence_score}, rates: {rates}\")\n",
    "        # merge_scores = 0.999*np.array(new_sequence_score) + 0.001*np.array(rates)\n",
    "        # else:\n",
    "        exp_new_sequence_score = [np.exp(score) for score in new_sequence_score]\n",
    "        exp_rates = [np.exp(rate) for rate in rates]\n",
    "        norm_exp_new_sequence_score = exp_new_sequence_score / np.sum(exp_new_sequence_score)\n",
    "        norm_exp_rates = exp_rates / np.sum(exp_rates)\n",
    "        log_norm_exp_new_sequence_score = [np.log(score) for score in norm_exp_new_sequence_score]\n",
    "        log_norm_exp_rates = [np.log(rate) for rate in norm_exp_rates]\n",
    "\n",
    "        # print(\"log_norm_exp_new_sequence_score: \", log_norm_exp_new_sequence_score)\n",
    "        # print(\"log_norm_exp_rates: \", log_norm_exp_rates)\n",
    "        # print()\n",
    "\n",
    "        # merge_scores = 0.9*np.array(log_norm_exp_new_sequence_score) + 0.1*np.array(log_norm_exp_rates)\n",
    "        merge_scores = new_sequence_score\n",
    "        # merge_scores = rates\n",
    "\n",
    "        '''\n",
    "        the way to merge the two modules for post processng, v2\n",
    "        '''\n",
    "        # check if the first action predicted by policy model is in the possible subsequent actions of the previous action, if not, check the second action, and so on\n",
    "        # if none of the actions predicted by the policy model is in the possible subsequent actions of the previous action, then use most probable action from the prior\n",
    "        # merge_scores = []\n",
    "        all_possible_actions = ['search-faq', 'search-timing', 'pull-up-account', 'verify-identity', 'membership', 'ask-the-oracle', 'search-shirt', 'search-policy', 'select-faq', 'send-link', 'enter-details', 'log-out-in', 'promo-code', 'notify-team', 'make-purchase', 'validate-purchase', 'update-order', 'subscription-status', 'make-password', 'try-again', 'shipping-status', 'record-reason', 'update-account', 'instructions', 'search-boots', 'search-jeans', 'search-membership', 'search-jacket', 'search-pricing', 'offer-refund']\n",
    "\n",
    "        current_action_probs = {}\n",
    "        for possible_action in all_possible_actions:\n",
    "            if transition_mapping[(previous_actions[-1], possible_action)][0] != 'unknown':\n",
    "                current_action_probs[possible_action] = transition_mapping[(previous_actions[-1], possible_action)][1]\n",
    "\n",
    "\n",
    "        # if new_pred[0].split(' ')[0].strip() not in current_action_probs:\n",
    "        #     not_in_counter += 1\n",
    "        #     if new_pred[1].split(' ')[0].strip() in current_action_probs:\n",
    "        #         new_new_predictions.append(new_pred[1])\n",
    "        #         print(f\"2: wrong: {new_pred[0].split(' ')[0].strip()}, correct: {new_pred[1].split(' ')[0].strip()}, label: {label1.split(' ')[0].strip()}\")\n",
    "        #     else:\n",
    "        #         max_index = np.argmax(list(current_action_probs.values()))\n",
    "        #         new_new_predictions.append(list(current_action_probs.keys())[max_index] + ' ' + new_pred[0].split(' ')[1].strip())\n",
    "        #         print(f\"chain: wrong: {new_pred[i].split(' ')[0].strip()}, correct: {list(current_action_probs.keys())[max_index]}, label: {label1.split(' ')[0].strip()}\")\n",
    "        # else:\n",
    "        max_index = np.argmax(merge_scores)\n",
    "        new_new_predictions.append(new_pred[max_index])\n",
    "\n",
    "        previous_actions.append(label1.split(' ')[0].strip())\n",
    "\n",
    "    print(f\"not_in_counter: {not_in_counter}, total number of predictions: {len(new_predictions)}\")\n",
    "\n",
    "    \"\"\"Adapted from ABCD. \"\"\"\n",
    "    # print(\"predictions:\", predictions)\n",
    "    # print(\"labels:\", labels)\n",
    "    action_preds = []\n",
    "    action_labels = []\n",
    "\n",
    "    value_preds = []\n",
    "    value_labels = []\n",
    "    \n",
    "    # print(\"len(new_new_predictions): \", len(new_new_predictions))\n",
    "    # print(\"len(labels): \", len(labels))\n",
    "    for pred, label in zip(new_new_predictions, labels):\n",
    "        action_label, values_label = parse_ast_prediction(label)\n",
    "        values_label.sort()\n",
    "        # for value in values_label:\n",
    "        #     action_labels.append(action_label)\n",
    "        #     value_labels.append(value)\n",
    "        action_labels.append(action_label)\n",
    "        value_labels.append(values_label)\n",
    "\n",
    "        # print(\"pred str: \", pred)\n",
    "        action_pred, values_pred = parse_ast_prediction(pred)\n",
    "        # print(f\"parsed results: {action_pred}, {values_pred}\")\n",
    "        values_pred.sort()\n",
    "\n",
    "        if len(values_pred) > len(values_label):\n",
    "            values_pred = [v for v in values_label if v in values_pred]\n",
    "        if len(values_pred) < len(values_label):\n",
    "            values_pred.extend([\"MISSING\"] * (len(values_label) - len(values_pred)))\n",
    "\n",
    "        # for value in values_pred:\n",
    "        #     action_preds.append(action_pred)\n",
    "        #     value_preds.append(value)\n",
    "        action_preds.append(action_pred)\n",
    "        value_preds.append(values_pred)\n",
    "\n",
    "    # print(\"action_preds: \", action_preds)\n",
    "    # print(\"action_labels: \", action_labels)\n",
    "    # print(\"value_labels: \", value_labels)\n",
    "    # print(\"convo_ids: \", convo_ids)\n",
    "    # print(\"turn_ids: \", turn_ids)\n",
    "\n",
    "    action_labels_arrary = np.array(action_labels, dtype=object)\n",
    "    action_preds_arrary = np.array(action_preds, dtype=object)\n",
    "    # print(f\"action_labels_arrary: {action_labels_arrary}\")\n",
    "    # print(f\"action_preds_arrary: {action_preds_arrary}\")\n",
    "    action_match = action_labels_arrary == action_preds_arrary\n",
    "\n",
    "    # plot confusion matrix\n",
    "    # print(f\"action_labels_arrary: {action_labels_arrary}\")\n",
    "    # print(f\"action_preds_arrary: {action_preds_arrary}\")\n",
    "    '''\n",
    "    acc_actions: {'search-faq': [206, 242], 'search-timing': [26, 30], 'pull-up-account': [675, 709], 'verify-identity': [329, 373], 'membership': [114, 136], 'ask-the-oracle': [147, 171], 'search-shirt': [32, 34], 'search-policy': [31, 44], 'select-faq': [168, 171], 'send-link': [52, 57], 'enter-details': [185, 211], 'log-out-in': [85, 92], 'promo-code': [49, 53], 'notify-team': [70, 75], 'make-purchase': [56, 59], 'validate-purchase': [211, 233], 'update-order': [120, 142], 'subscription-status': [36, 53], 'make-password': [40, 42], 'try-again': [44, 60], 'shipping-status': [61, 86], 'record-reason': [137, 172], 'update-account': [77, 92], 'instructions': [40, 49], 'search-boots': [23, 25], 'search-jeans': [24, 26], 'search-membership': [25, 35], 'search-jacket': [22, 27], 'search-pricing': [24, 36], 'offer-refund': [59, 73]}\n",
    "    '''\n",
    "    possibleActions = ['search-faq', 'search-timing', 'pull-up-account', 'verify-identity', 'membership', 'ask-the-oracle', 'search-shirt', 'search-policy', 'select-faq', 'send-link', 'enter-details', 'log-out-in', 'promo-code', 'notify-team', 'make-purchase', 'validate-purchase', 'update-order', 'subscription-status', 'make-password', 'try-again', 'shipping-status', 'record-reason', 'update-account', 'instructions', 'search-boots', 'search-jeans', 'search-membership', 'search-jacket', 'search-pricing', 'offer-refund', 'MISSING']\n",
    "\n",
    "\n",
    "    action_acc = sum(action_match) / float(len(action_labels))\n",
    "\n",
    "    value_labels_arrary = np.array(value_labels, dtype=object)\n",
    "    value_preds_arrary = np.array(value_preds, dtype=object)\n",
    "    # print(f\"value_labels_arrary: {value_labels_arrary}\")\n",
    "    # print(f\"value_preds_arrary: {value_preds_arrary}\")\n",
    "    value_match = value_labels_arrary == value_preds_arrary\n",
    "    # print(f\"value_match: {value_match}\")\n",
    "    value_acc = sum(value_match) / float(len(action_labels))\n",
    "\n",
    "    joint_match = action_match & value_match\n",
    "    joint_acc = sum(joint_match) / float(len(action_labels))\n",
    "\n",
    "    # group by convo_ids\n",
    "    unique_convo_ids = list(set(convo_ids))\n",
    "    # print(f\"unique_convo_ids: {unique_convo_ids}\")\n",
    "    conversations = {}\n",
    "    for uci in unique_convo_ids:\n",
    "        turns, correctness = [], []\n",
    "        correctness_action, correctness_value = [], []\n",
    "        row_id = 0\n",
    "        for convo_id, turn_count in zip(convo_ids, turn_ids):\n",
    "            if convo_id == uci:\n",
    "                turns.append(turn_count)\n",
    "                correct = False\n",
    "                correct_action = False\n",
    "                correct_value = False\n",
    "                action_right = action_match[row_id]\n",
    "                value_right = value_match[row_id]\n",
    "                # print(f\"action_right: {action_right}, value_right: {value_right}\")\n",
    "                \n",
    "                if action_right:\n",
    "                    correct_action = True\n",
    "                else:\n",
    "                    correct_action = False\n",
    "                \n",
    "                if value_right:\n",
    "                    correct_value = True\n",
    "                else:\n",
    "                    correct_value = False\n",
    "\n",
    "                if action_right and value_right:\n",
    "                    correct = True\n",
    "                else:\n",
    "                    correct = False\n",
    "\n",
    "                correctness.append(correct)\n",
    "                correctness_action.append(correct_action)\n",
    "                correctness_value.append(correct_value)\n",
    "            row_id += 1\n",
    "\n",
    "        # sort by turn_counts\n",
    "        ordered = [cor for _, cor in sorted(zip(turns, correctness), key=lambda tc: tc[0])]\n",
    "        ordered_action = [cor for _, cor in sorted(zip(turns, correctness_action), key=lambda tc: tc[0])]\n",
    "        ordered_value = [cor for _, cor in sorted(zip(turns, correctness_value), key=lambda tc: tc[0])]\n",
    "        conversations[uci] = [ordered, ordered_action, ordered_value]\n",
    "\n",
    "    # count how many correct\n",
    "    turn_score, turn_correct = 0, 0\n",
    "    turn_score_action, turn_correct_action = 0, 0\n",
    "    turn_score_value, turn_correct_value = 0, 0\n",
    "    em_joint, em_action, em_value = [], [], []\n",
    "    my_scores = []\n",
    "    for convo_id, itm in conversations.items():\n",
    "        # print(f\"convo_id: {convo_id}\")\n",
    "        convo_correctness = itm[0]\n",
    "        convo_correctness_action = itm[1]\n",
    "        convo_correctness_value = itm[2]\n",
    "\n",
    "        # calculate EM\n",
    "        if sum(convo_correctness) == len(convo_correctness):\n",
    "            em_joint.append(True)\n",
    "        else:\n",
    "            em_joint.append(False)\n",
    "        if sum(convo_correctness_action) == len(convo_correctness_action):\n",
    "            em_action.append(True)\n",
    "        else:\n",
    "            em_action.append(False)\n",
    "        if sum(convo_correctness_value) == len(convo_correctness_value):\n",
    "            em_value.append(True)\n",
    "        else:\n",
    "            em_value.append(False)\n",
    "        \n",
    "        # print(f\"convo_id: {convo_id}, convo_correctness: {convo_correctness}\")\n",
    "        current_score = 0\n",
    "        convo_length = len(convo_correctness)\n",
    "        # we use turn_id rather than the true turn_count since turn counts will skip numbers\n",
    "        # when looping through the conversation due to skipping over customer utterances\n",
    "\n",
    "        snipet_lens = [1,2,3]\n",
    "\n",
    "        # for joint correctness\n",
    "        snipet_lens_joint  = snipet_lens\n",
    "        snipet_numbers_joint = [0] * len(snipet_lens_joint)\n",
    "        snipet_correct_joint = [0] * len(snipet_lens_joint)\n",
    "        # for each dialogue, compute the rate of each length of snipet that is correct, using the sliding window of the length\n",
    "        for snipet_i in range(len(snipet_lens_joint)):\n",
    "            # print(\"convo_length: \", convo_length)\n",
    "            if snipet_lens_joint[snipet_i] > convo_length:\n",
    "                continue\n",
    "            # print(f\"snipet_i: {snipet_i}\")\n",
    "            snipet_len = snipet_lens_joint[snipet_i]\n",
    "            for turn_id in range(convo_length - snipet_len + 1):\n",
    "                snipet_numbers_joint[snipet_i] += 1\n",
    "                if sum(convo_correctness[turn_id:turn_id+snipet_len]) == snipet_len:\n",
    "                    snipet_correct_joint[snipet_i] += 1\n",
    "\n",
    "        average_counter = 0\n",
    "        for snipet_i in range(len(snipet_lens_joint)):\n",
    "            # print(f\"snipet_correct_joint: {snipet_correct_joint[snipet_i]}, snipet_numbers_joint: {snipet_numbers_joint[snipet_i]}\")\n",
    "            if snipet_numbers_joint[snipet_i] == 0:\n",
    "                continue\n",
    "            snipet_correct_joint[snipet_i] = snipet_correct_joint[snipet_i] / snipet_numbers_joint[snipet_i]\n",
    "            average_counter += 1\n",
    "        \n",
    "        # print(f\"snipet_correct: {snipet_correct_joint}\")\n",
    "        # print(\"average_counter: \", average_counter)\n",
    "        average_for_dialogue = 0\n",
    "        for snipet_i in range(len(snipet_lens_joint)):\n",
    "            average_for_dialogue += snipet_correct_joint[snipet_i]\n",
    "        average_for_dialogue = average_for_dialogue / len(snipet_lens)\n",
    "        # average_for_dialogue = average_for_dialogue / average_counter\n",
    "        # print(f\"average_for_dialogue: {average_for_dialogue}\")\n",
    "\n",
    "        turn_score += average_for_dialogue\n",
    "\n",
    "        # for action correctness\n",
    "        snipet_lens_action  = snipet_lens\n",
    "        snipet_numbers_action = [0] * len(snipet_lens_action)\n",
    "        snipet_correct_action = [0] * len(snipet_lens_action)\n",
    "        # for each dialogue, compute the rate of each length of snipet that is correct, using the sliding window of the length\n",
    "        for snipet_i in range(len(snipet_lens_action)):\n",
    "            # print(\"convo_length: \", convo_length)\n",
    "            if snipet_lens_action[snipet_i] > convo_length:\n",
    "                continue\n",
    "            # print(f\"snipet_i: {snipet_i}\")\n",
    "            snipet_len = snipet_lens_action[snipet_i]\n",
    "            for turn_id in range(convo_length - snipet_len + 1):\n",
    "                snipet_numbers_action[snipet_i] += 1\n",
    "                if sum(convo_correctness_action[turn_id:turn_id+snipet_len]) == snipet_len:\n",
    "                    snipet_correct_action[snipet_i] += 1\n",
    "\n",
    "        average_counter = 0\n",
    "        for snipet_i in range(len(snipet_lens_action)):\n",
    "            if snipet_numbers_action[snipet_i] == 0:\n",
    "                continue\n",
    "            snipet_correct_action[snipet_i] = snipet_correct_action[snipet_i] / snipet_numbers_action[snipet_i]\n",
    "            average_counter += 1\n",
    "\n",
    "        # print(f\"snipet_correct: {snipet_correct_action}\")\n",
    "        average_for_dialogue = 0\n",
    "        for snipet_i in range(len(snipet_lens_action)):\n",
    "            average_for_dialogue += snipet_correct_action[snipet_i]\n",
    "        average_for_dialogue = average_for_dialogue / len(snipet_lens)\n",
    "        # average_for_dialogue = average_for_dialogue / average_counter\n",
    "        # print(f\"average_for_dialogue: {average_for_dialogue}\")\n",
    "\n",
    "        turn_score_action += average_for_dialogue\n",
    "\n",
    "        # for value correctness\n",
    "        snipet_lens_value  = snipet_lens\n",
    "        snipet_numbers_value = [0] * len(snipet_lens_value)\n",
    "        snipet_correct_value = [0] * len(snipet_lens_value)\n",
    "        # for each dialogue, compute the rate of each length of snipet that is correct, using the sliding window of the length\n",
    "        for snipet_i in range(len(snipet_lens_value)):\n",
    "            # print(\"convo_length: \", convo_length)\n",
    "            if snipet_lens_value[snipet_i] > convo_length:\n",
    "                continue\n",
    "            # print(f\"snipet_i: {snipet_i}\")\n",
    "            snipet_len = snipet_lens_value[snipet_i]\n",
    "            for turn_id in range(convo_length - snipet_len + 1):\n",
    "                snipet_numbers_value[snipet_i] += 1\n",
    "                if sum(convo_correctness_value[turn_id:turn_id+snipet_len]) == snipet_len:\n",
    "                    snipet_correct_value[snipet_i] += 1\n",
    "\n",
    "        average_counter = 0\n",
    "        for snipet_i in range(len(snipet_lens_value)):\n",
    "            if snipet_numbers_value[snipet_i] == 0:\n",
    "                continue\n",
    "            snipet_correct_value[snipet_i] = snipet_correct_value[snipet_i] / snipet_numbers_value[snipet_i]\n",
    "            average_counter += 1\n",
    "\n",
    "        # print(f\"snipet_correct: {snipet_correct_value}\")\n",
    "        average_for_dialogue = 0\n",
    "        for snipet_i in range(len(snipet_lens_value)):\n",
    "            average_for_dialogue += snipet_correct_value[snipet_i]\n",
    "        average_for_dialogue = average_for_dialogue / len(snipet_lens)\n",
    "        # average_for_dialogue = average_for_dialogue / average_counter\n",
    "        # print(f\"average_for_dialogue: {average_for_dialogue}\")\n",
    "\n",
    "        turn_score_value += average_for_dialogue\n",
    "\n",
    "    # normalize by total number of turns possible\n",
    "    '''\n",
    "    len(convo_ids): 200, len(turn_ids): 200\n",
    "    '''\n",
    "    # print(f\"len(convo_ids): {len(convo_ids)}, len(turn_ids): {len(turn_ids)}\")\n",
    "    turn_acc = turn_correct / float(len(conversations))\n",
    "    turn_acc_action = turn_correct_action / float(len(conversations))\n",
    "    turn_acc_value = turn_correct_value / float(len(conversations))\n",
    "    final_score = turn_score / float(len(conversations))\n",
    "    final_score_action = turn_score_action / float(len(conversations))\n",
    "    final_score_value = turn_score_value / float(len(conversations))\n",
    "    \n",
    "    em_action_score = sum(em_action) / float(len(em_action))\n",
    "    em_value_score = sum(em_value) / float(len(em_value))\n",
    "    em_joint_score = sum(em_joint) / float(len(em_joint))\n",
    "\n",
    "    return {\n",
    "        \"EM_action\": round(em_action_score, 4),\n",
    "        \"EM_value\": round(em_value_score, 4),\n",
    "        \"EM_joint\": round(em_joint_score, 4),\n",
    "        # \"turn_acc_joint\": round(turn_acc, 4),\n",
    "        # \"turn_acc_action\": round(turn_acc_action, 4),\n",
    "        # \"turn_acc_value\": round(turn_acc_value, 4),\n",
    "        \"CE_joint\": round(final_score, 4),\n",
    "        \"CE_action\": round(final_score_action, 4),\n",
    "        \"CE_value\": round(final_score_value, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### can call LLM for generating response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_prompt = \"\"\"\n",
    "The following are conversations between a user and an assistant. Indicated by the dialog acts, the assistant can help the user with checking in or providing information of temperature, time, price, location, and so on.\n",
    "The response should be coherent, engaging, diverse, informative, and overall good and should be in line with the next action.\n",
    "The response should be concise and to the point and not exceed 30 words.\n",
    "\n",
    "Available Dialog acts:\n",
    "confirm appointment_date, confirm appointment_time, confirm check_in_date, confirm date, confirm destination, confirm event_name, confirm location, confirm number_of_rooms, confirm number_of_seats, confirm pickup_location, confirm pickup_time, confirm property_name, confirm restaurant_name, confirm time, confirm visit_date, inform address, inform category, inform date, inform free_entry, inform from_station, inform genre, inform good_for_kids, inform humidity, inform inbound_arrival_time, inform outbound_arrival_time, inform phone_number, inform price, inform rating, inform ride_fare, inform time, inform to_station, inform wind, inform_count count, offer address, offer airlines, offer album, offer appointment_date, offer appointment_time, offer artist, offer attraction_name, offer car_name, offer category, offer location, offer date, offer event_name, offer inbound_departure_time, offer movie_name, offer outbound_departure_time, offer pickup_location, offer precipitation, offer price, offer property_name, offer restaurant_name, offer rating, offer temperature, offer time, offer title, offer type, offer_intent intent, request appointment_date, request appointment_time, request area, request category, request check_in_date, request date, request departure_date, request destination, request event_name, request genre, request location, request number_of_beds, request number_of_seats, request pickup_time, request restaurant_name, request return_date, request time, request type, request visit_date, notify_failure, notify_success, req_more, goodbye.\n",
    "\n",
    "Conversation: \n",
    "User: i want to make a restaurant reservation for 2 people at half past 11 in the morning.  \n",
    "Assistant(request restaurant_name [none], request location [none]): what city do you want to dine in? do you have a preferred restaurant? \n",
    "User: please find restaurants in san jose. can you try sino?  \n",
    "Assistant(confirm restaurant_name [sino]. confirm location [san jose]. confirm time [11:30 am]. confirm number_of_seats [2]. confirm date [today]): confirming: i will reserve a table for 2 people at sino in san jose. the reservation time is 11:30 am today. \n",
    "User: yes, thanks. what's their phone number?  \n",
    "Assistant(inform phone_number [408-247-8880], notify_success [none]): your reservation has been made. their phone number is 408-247-8880. \n",
    "User: what's their address? do they have vegetarian options on their menu?  \n",
    "Assistant(inform address [377 santana row #1000]): the street address is 377 santana row #1000. they have good vegetarian options. \n",
    "User: thanks very much.  \n",
    "Assistant(req_more [none]): is there anything else i can help you with? \n",
    "User: no, that's all. thanks.\n",
    "\n",
    "Conversation:\n",
    "User: i'm looking for something like a performing arts venue to visit, can you help me?  \n",
    "Assistant(request location [none]): where would you like me to search? \n",
    "User: search in paris, france please.  \n",
    "Assistant(offer attraction_name [arab world institute], offer category [performing arts venue]): there's the arab world institute, which is a performing arts venue. \n",
    "User: can i bring children there, also is this place free to go to?  \n",
    "Assistant(inform good_for_kids [false], inform free_entry [false]): i don't think children would enjoy this venue, and it is not free. \n",
    "User: okay, cool. that's all i need for now, bye.\n",
    "\n",
    "Conversation:\n",
    "User: hi there, could you find some pop music events for me? \n",
    "Assistant: where are you interested in? \n",
    "User: something in philly is fine.  \n",
    "Assistant(offer event_name [conan gray], offer date [march 1st], offer time [8:30 pm]): would you like conan gray event at the fillmore philadelphia occurring on march 1st at 8:30 pm? \n",
    "User: sure, could you find something else on march 4th in sd instead?  \n",
    "Assistant(offer event_name [tuxedo], offer date [march 4th], offer time [7 pm], inform_count count [1]): there is 1 event called tuxedo at the observatory north park at 7 pm on march 4th. \n",
    "User: sure, that sounds good!  \n",
    "Assistant(offer_intent intent [buyeventtickets]): would you like me to purchase the tickets? \n",
    "User: perhaps later!  \n",
    "Assistant(req_more [none]): could i further assist you? \n",
    "User: could you get me a bus ride there too? \n",
    "Assistant: sure, do you intend to depart on march 4th? \n",
    "User: no, i will leave on march 3rd instead. \n",
    "Assistant: and where will you be travelling from? \n",
    "User: i will travel from lax.  \n",
    "Assistant(inform_count count [10]): there are 10 available buses, would you like one which departs at 7:40 am with 0 transfers costing $28? \n",
    "User: that sounds great! could you get me a roundtrip flight there too?  \n",
    "Assistant(request return_date [none]): sure, when will you be returning? \n",
    "User: i will return next thursday.  \n",
    "Assistant(request departure_date [march 3rd]): and will you depart on march 3rd or march 4th. \n",
    "User: preferably march 3rd, please.  \n",
    "Assistant(offer airlines [alaska airlines], offer outbound_departure_time [6:55 am], offer inbound_departure_time [9:55 am], offer price [$116]): how about an alaska airlines flight departing at 6:55 am and returning at 9:55 am costing $116? \n",
    "User: are there any alternatives?  \n",
    "Assistant(offer airlines [american airlines], offer outbound_departure_time [8:55 pm], offer inbound_departure_time [7:10 pm], offer price [$127]): how about an american airlines flight departing at 8:55 pm and returning at 7:10 pm costing $127? \n",
    "User: which airport will i depart from? \n",
    "Assistant: you will depart from lax international airport. \n",
    "User: sure, could you get me hotels in that area too?  \n",
    "Assistant(offer rating [3]): how about a 3 star hotel called bahia resort hotel? \n",
    "User: are there any alternatives?  \n",
    "Assistant(offer rating [2]): how about a 2 star hotel called best western mission bay? \n",
    "User: that sounds great too!  \n",
    "Assistant(offer_intent intent [reservehotel]): would you like me to make a reservation? \n",
    "User: no thanks, that is not needed.\n",
    "\n",
    "Conversation: \n",
    "[[DIALOG]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "\n",
    "def call_LLM(dialogue, Action):\n",
    "\n",
    "    prompt = hint_prompt.replace(\"[[DIALOG]]\", dialogue)\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. You can generate a response to the user's input based on the given previous dialogue and the next action.\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = clientGPT3_5.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.9,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "data = []\n",
    "with open('/research/d5/gds/xywen22/project/llm_framework/AST_abcd_part/data/processed/test_AST_abcd_woaction_flow_all.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_data = json.loads(line)\n",
    "        data.append(json_data)\n",
    "\n",
    "context_list = []\n",
    "for dialogue_i in range(len(data)):\n",
    "    if dialogue_i == 10:\n",
    "        break\n",
    "    user_input = data[dialogue_i]['input']\n",
    "    context = user_input\n",
    "    # save the context to a tmp json file:\n",
    "    # {\"sample_id\": 0, \"target\": \"request time [none]\", \"input\": \"Context: hi, could you get me a restaurant booking on the 8th please? \", \"target_data\": \"[\\\"request time\\\", [\\\"none\\\"]]\"}\n",
    "    save_context = {\"sample_id\": 0, \"convo_id\": data[dialogue_i]['convo_id'], \"turn_id\": data[dialogue_i]['turn_id'], \"target\": data[dialogue_i]['target'], \"input\": context, \"target_data\": data[dialogue_i]['target_data']}\n",
    "    if os.path.exists(\"tmp.json\"):\n",
    "        os.remove(\"tmp.json\")\n",
    "    # print(tmp_sample)\n",
    "    with open(f\"tmp.json\", \"a\") as w:\n",
    "        json.dump(save_context, w)\n",
    "        w.write(\"\\n\")\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = datasets_loader.load_datasets()\n",
    "    print(f\"num_beams: {num_beams}\")\n",
    "    result_pred, result_label = do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams)\n",
    "\n",
    "    # Call the LLM model to generate the response\n",
    "    print(\"all predictions: \", result_pred)\n",
    "    action = result_pred[-1]\n",
    "\n",
    "    # build the context for the next turn for calling the LLM model\n",
    "    dialog_with_hint = \"\"\n",
    "    for each in context_list:\n",
    "        dialog_with_hint += \"User: \" + each[\"user\"] + \"\\n\" + \"Assistant(\" + each[\"action\"] + \"): \" + each[\"agent\"] + \"\\n\"\n",
    "    dialog_with_hint += \"User: \" + user_input + \"\\n\" + \"Assistant(\" + action + \"): \"\n",
    "\n",
    "    response = call_LLM(dialog_with_hint, action)\n",
    "    print(\"User: \", user_input)\n",
    "    print(\"** Next Action **: \", action)\n",
    "    print(\"Agent: \", response)\n",
    "    context += str(action) + \". \"\n",
    "    if response[-1] not in [\".\", \"?\", \"!\"]:\n",
    "        response += \".\"\n",
    "    context += str(response) + \" \"\n",
    "    context_list.append({\"user\": user_input, \"action\": action, \"agent\": response})\n",
    "    print(\"*\" *100)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"context: \", context)\n",
    "print(\"context_list: \", context_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only generate the action list, not the full dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# import json\n",
    "\n",
    "# data = []\n",
    "# with open('/research/d5/gds/xywen22/project/llm_framework/AST_abcd_part/data/processed/test_AST_abcd_woaction_flow_all.json', 'r') as file:\n",
    "#     for line in file:\n",
    "#         json_data = json.loads(line)\n",
    "#         data.append(json_data)\n",
    "\n",
    "# context_list = []\n",
    "# distinct_dialogue = {}\n",
    "# distinct_dialogue[\"dialogue\"] = []\n",
    "# distinct_dialogue[\"pred_action_value\"] = []\n",
    "# distinct_dialogue[\"action_value_label\"] = []\n",
    "# distinct_dialogue[\"convo_ids\"] = []\n",
    "# distinct_dialogue[\"turn_ids\"] = []\n",
    "# current_conv_id = 0\n",
    "# counter_success_dialogues = 0\n",
    "\n",
    "# if os.path.exists(\"data/updating/incremental_data.json\"):\n",
    "#     # remove the file\n",
    "#     os.remove(\"data/updating/incremental_data.json\")\n",
    "\n",
    "# for dialogue_i in range(len(data)):\n",
    "#     user_input = data[dialogue_i]['input']\n",
    "#     context = user_input\n",
    "#     # save the context to a tmp json file:\n",
    "#     # {\"sample_id\": 0, \"target\": \"request time [none]\", \"input\": \"Context: hi, could you get me a restaurant booking on the 8th please? \", \"target_data\": \"[\\\"request time\\\", [\\\"none\\\"]]\"}\n",
    "#     save_context = {\"sample_id\": data[dialogue_i]['sample_id'], \"convo_id\": data[dialogue_i]['convo_id'], \"turn_id\": data[dialogue_i]['turn_id'], \"target\": data[dialogue_i]['target'], \"input\": context, \"target_data\": data[dialogue_i]['target_data']}\n",
    "#     if os.path.exists(\"tmp.json\"):\n",
    "#         os.remove(\"tmp.json\")\n",
    "#     # print(tmp_sample)\n",
    "#     with open(f\"tmp.json\", \"a\") as w:\n",
    "#         json.dump(save_context, w)\n",
    "#         w.write(\"\\n\")\n",
    "\n",
    "#     train_dataset, validation_dataset, test_dataset = datasets_loader.load_datasets()\n",
    "#     result_pred, result_label = do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams)\n",
    "\n",
    "#     # Call the LLM model to generate the response\n",
    "#     action = result_pred[-1]\n",
    "\n",
    "#     action = postprocess_predictions(action)\n",
    "\n",
    "#     print(\"context: \", context)\n",
    "#     print(\"agent: \", action)\n",
    "#     print(\"gold: \", data[dialogue_i]['target'])\n",
    "#     print(\"-\" * 30)\n",
    "#     print()\n",
    "\n",
    "#     if data[dialogue_i]['convo_id'] != current_conv_id:\n",
    "#         if current_conv_id == 0:\n",
    "#             pass\n",
    "#         else:\n",
    "#             # calculate the CE metric\n",
    "#             metrics = compute_ast_acc_metrics(distinct_dialogue[\"pred_action_value\"], distinct_dialogue[\"action_value_label\"], distinct_dialogue[\"convo_ids\"], distinct_dialogue[\"turn_ids\"])\n",
    "            \n",
    "#             # print(\"CE_joint: \", metrics[\"CE_joint\"])\n",
    "#             # print(\"CE_action: \", metrics[\"CE_action\"])\n",
    "#             # print(\"CE_value: \", metrics[\"CE_value\"])\n",
    "#             if metrics[\"CE_joint\"] > 0.5 and metrics[\"CE_action\"] > 0.5 and metrics[\"CE_value\"] > 0.5:\n",
    "#                 print(\"CE_joint: \", metrics[\"CE_joint\"])\n",
    "#                 print(\"CE_action: \", metrics[\"CE_action\"])\n",
    "#                 print(\"CE_value: \", metrics[\"CE_value\"])\n",
    "#                 print(\"EM action: \", metrics[\"EM_action\"])\n",
    "#                 print(\"EM value: \", metrics[\"EM_value\"])\n",
    "#                 print(\"EM joint: \", metrics[\"EM_joint\"])\n",
    "#                 print(distinct_dialogue[\"pred_action_value\"])\n",
    "#                 print(distinct_dialogue[\"action_value_label\"])\n",
    "\n",
    "#                 counter_success_dialogues += 1\n",
    "\n",
    "#                 for i in range(len(distinct_dialogue[\"dialogue\"])):\n",
    "#                     # print(distinct_dialogue[\"dialogue\"][i][\"input\"])\n",
    "#                     # print(distinct_dialogue[\"dialogue\"][i][\"predicted_action\"])\n",
    "#                     # print(distinct_dialogue[\"dialogue\"][i][\"target\"])\n",
    "#                     # print(\"-\" * 30)\n",
    "#                     with open(\"data/updating/incremental_data.json\", \"a\") as w:\n",
    "#                         json.dump(distinct_dialogue[\"dialogue\"][i], w)\n",
    "#                         w.write(\"\\n\")\n",
    "\n",
    "#                 if counter_success_dialogues == 2:\n",
    "#                     break\n",
    "\n",
    "#         distinct_dialogue[\"dialogue\"] = []\n",
    "#         distinct_dialogue[\"pred_action_value\"] = []\n",
    "#         distinct_dialogue[\"action_value_label\"] = []\n",
    "#         distinct_dialogue[\"convo_ids\"] = []\n",
    "#         distinct_dialogue[\"turn_ids\"] = []\n",
    "\n",
    "#         save_context['predicted_action'] = action\n",
    "#         distinct_dialogue[\"dialogue\"].append(save_context)\n",
    "#         distinct_dialogue[\"pred_action_value\"].append(action)\n",
    "#         distinct_dialogue[\"action_value_label\"].append(data[dialogue_i]['target'])\n",
    "#         distinct_dialogue[\"convo_ids\"].append(data[dialogue_i]['convo_id'])\n",
    "#         distinct_dialogue[\"turn_ids\"].append(data[dialogue_i]['turn_id'])\n",
    "#         current_conv_id = data[dialogue_i]['convo_id']\n",
    "#     else:\n",
    "#         save_context['predicted_action'] = action\n",
    "#         distinct_dialogue[\"dialogue\"].append(save_context)\n",
    "#         distinct_dialogue[\"pred_action_value\"].append(action)\n",
    "#         distinct_dialogue[\"action_value_label\"].append(data[dialogue_i]['target'])\n",
    "#         distinct_dialogue[\"convo_ids\"].append(data[dialogue_i]['convo_id'])\n",
    "#         distinct_dialogue[\"turn_ids\"].append(data[dialogue_i]['turn_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "with open('/research/d5/gds/xywen22/project/llm_framework/AST_abcd_part/data/raw/abcd_v1.1.json', 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        json_data = json.loads(line)\n",
    "        data.append(json_data)\n",
    "        break\n",
    "\n",
    "print(f\"data: {data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_up_account = [\n",
    "    \"account has been pulled up for albert sanders.\",\n",
    "    \"account has been pulled up for sanya afzal.\"\n",
    "]\n",
    "\n",
    "enter-details = [\n",
    "    \"details of <username> have been entered.\",\n",
    "    \"details of 14 have been entered.\"\n",
    "]\n",
    "\n",
    "verify-identity= [\n",
    "    \"identity verification in progress ...\"\n",
    "]\n",
    "\n",
    "make-password = [\n",
    "    \"a password has been generated.\"\n",
    "]\n",
    "\n",
    "search-timing = [\n",
    "    \"system action: search timing\"\n",
    "]\n",
    "\n",
    "search-policy = [\n",
    "    \"system action: search policy\"\n",
    "]\n",
    "\n",
    "validate-purchase = [\n",
    "    \"purchase validation in progress ...\"\n",
    "]\n",
    "\n",
    "search-faq = [\n",
    "    \"searching the faq pages ...\"\n",
    "]\n",
    "\n",
    "membership = [\n",
    "    \"membership level of bronze has been noted.\"\n",
    "    \"membership level of guest has been noted.\"\n",
    "]\n",
    "\n",
    "search-boots = [\n",
    "    \"system action: search boots\",\n",
    "\n",
    "]\n",
    "\n",
    "try-again = [\n",
    "    \"agent is looking for solutions ...\"\n",
    "]\n",
    "\n",
    "ask-the-oracle = [\n",
    "    \"querying the system for an answer ...\"\n",
    "\n",
    "]\n",
    "\n",
    "update-order = [\n",
    "    \"order has been updated with change address.\"\n",
    "]\n",
    "\n",
    "promo-code = [\n",
    "    \"a promo code has been created.\"\n",
    "]\n",
    "\n",
    "update-account = [\n",
    "    \"account has been updated with renew subscription.\",\n",
    "    \"account has been updated with change time.\"\n",
    "]\n",
    "\n",
    "search-membership = [\n",
    "    \"system action: search membership\"\n",
    "]\n",
    "\n",
    "make-purchase = [\n",
    "    \"a purchase of <name> was made.\",\n",
    "    \"a purchase of calvin klein jacket was made.\"\n",
    "]\n",
    "\n",
    "offer-refund = [\n",
    "    \"a refund has been made for the amount of $<amount>.\",\n",
    "    \"a refund has been made for the amount of $1<amount>.\",\n",
    "]\n",
    "\n",
    "notify-team = [\n",
    "    \"the website team has been notified.\",\n",
    "]\n",
    "\n",
    "record-reason = [\n",
    "    \"a reason of paypal has been recorded.\",\n",
    "    \"a reason of competitor has been recorded.\"\n",
    "    \"a reason of spouse has been recorded.\"\n",
    "]\n",
    "\n",
    "search-jeans = [\n",
    "    \"system action: search jeans\"\n",
    "]\n",
    "\n",
    "shipping-status = [\n",
    "    \"shipping status of delivered has been noted.\",\n",
    "    \"shipping status of order received has been noted.\"\n",
    "]\n",
    "\n",
    "search-shirt = [\n",
    "    \"system action: search shirt\"\n",
    "]\n",
    "\n",
    "instructions = [\n",
    "    \"agent is looking for solutions ...\"\n",
    "]\n",
    "\n",
    "search-jacket = [\n",
    "    \"system action: search jacket\"\n",
    "]\n",
    "\n",
    "log-out-in = [\n",
    "    \"agent is looking for solutions ...\"\n",
    "]\n",
    "\n",
    "select-faq = [\n",
    "    \"faq answer related to boots (how1) was selected.\"\n",
    "    \"faq answer related to shirt (other2) was selected.\"\n",
    "]\n",
    "\n",
    "subscription-status = [\n",
    "    \"querying the system for subscription status ...\"\n",
    "]\n",
    "\n",
    "send-link = [\n",
    "    \"a link will be sent.\"\n",
    "]\n",
    "\n",
    "search-pricing = [\n",
    "    \"system action: search pricing\"\n",
    "]\n",
    "\n",
    "# summarize to a dict\n",
    "action_description = {\n",
    "    \"pull-up-account\": \"account has been pulled up for <name>.\",\n",
    "    \"enter-details\": \"details of <username> have been entered.\",\n",
    "    \"verify-identity\": \"identity verification in progress ...\",\n",
    "    \"make-password\": \"a password has been generated.\",\n",
    "    \"search-timing\": \"system action: search timing, I need to ask a certain question about timing.\",\n",
    "    \"search-policy\": \"system action: search policy, what kind of policy does the customer want to know?\",\n",
    "    \"validate-purchase\": \"purchase validation in progress ...\",\n",
    "    \"search-faq\": \"Answers can be found in the faq pages, searching the faq pages ...\",\n",
    "    \"membership\": \"membership level of <level> has been noted.\",\n",
    "    \"search-boots\": \"system action: search boots, click the boots toggle switch\",\n",
    "    \"try-again\": \"agent is looking for solutions ...\",\n",
    "    \"ask-the-oracle\": \"querying the system for an answer ...\",\n",
    "    \"update-order\": \"order has been updated with <change>.\",\n",
    "    \"promo-code\": \"a promo code has been created.\",\n",
    "    \"update-account\": \"account has been updated with <change>.\",\n",
    "    \"search-membership\": \"system action: search membership, I need to know the membership level of the customer.\",\n",
    "    \"make-purchase\": \"a purchase of <item> was made.\",\n",
    "    \"offer-refund\": \"a refund has been made for the amount of $<amount>.\",\n",
    "    \"notify-team\": \"the website team has been notified.\",\n",
    "    \"record-reason\": \"a reason of <reason> has been recorded.\",\n",
    "    \"search-jeans\": \"system action: search jeans, click the jeans toggle switch\",\n",
    "    \"shipping-status\": \"shipping status of <status> has been noted.\",\n",
    "    \"search-shirt\": \"system action: search shirt, click the shirt toggle switch\",\n",
    "    \"instructions\": \"agent is looking for solutions ..., I will give you some instructions.\",\n",
    "    \"search-jacket\": \"system action: search jacket, click the jecket toggle switch\",\n",
    "    \"log-out-in\": \"agent is looking for solutions ..., instruct the customer to log out of their account and log back in.\",\n",
    "    \"select-faq\": \"faq answer related to <faq> was selected.\",\n",
    "    \"subscription-status\": \"querying the system for subscription status ...\",\n",
    "    \"send-link\": \"a link will be sent.\",\n",
    "    \"search-pricing\": \"system action: search pricing, price of something.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 98.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.05 seconds, 1757.35 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "from rouge import Rouge\n",
    "from bert_score import score\n",
    "import json\n",
    "\n",
    "def calculate_bert_score(dialogues):\n",
    "    references = []\n",
    "    candidates = []\n",
    "    for dialogue in dialogues:\n",
    "        references.append(dialogue[\"label_utterance\"])\n",
    "        candidates.append(dialogue[\"pred_utterance\"])\n",
    "\n",
    "    P, R, F1 = score(candidates, references, lang='en', verbose=True, rescale_with_baseline=True)\n",
    "    # mean P, R, F1\n",
    "    P = P.mean().item()\n",
    "    R = R.mean().item()\n",
    "    F1 = F1.mean().item()\n",
    "    return P, R, F1\n",
    "\n",
    "def calculate_BLEU_Score(dialogues):\n",
    "    references = []\n",
    "    candidates = []\n",
    "    for dialogue in dialogues:\n",
    "        references.append(dialogue[\"label_utterance\"])\n",
    "        candidates.append(dialogue[\"pred_utterance\"])\n",
    "\n",
    "    return sacrebleu.corpus_bleu(candidates, [references]).score\n",
    "\n",
    "\n",
    "def calculate_rouge_scores(dialogues):\n",
    "    hypothesis = []\n",
    "    reference = []\n",
    "    for dialogue in dialogues:\n",
    "        hypothesis.append(dialogue[\"pred_utterance\"])\n",
    "        reference.append(dialogue[\"label_utterance\"])\n",
    "    \n",
    "    rouge = Rouge()\n",
    "\n",
    "    scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "    return scores \n",
    "\n",
    "with open(\"dialogues/abcdASTWOActionFlowAll_wo_chainedPrior.json\", \"r\") as r:\n",
    "    dialogues = [json.loads(line) for line in r]\n",
    "\n",
    "dialog_BLEU = calculate_BLEU_Score(dialogues)\n",
    "dialog_ROUGE = calculate_rouge_scores(dialogues)\n",
    "dialog_BERT = calculate_bert_score(dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialog_BLEU:  34.97804356890443\n",
      "dialog_ROUGE:  {'rouge-1': {'r': 0.6210459256190967, 'p': 0.5383475930749909, 'f': 0.5649412970641914}, 'rouge-2': {'r': 0.5387776203532834, 'p': 0.45980727537952437, 'f': 0.48421015696692354}, 'rouge-l': {'r': 0.6186531625556021, 'p': 0.5364894791571523, 'f': 0.5629253138646001}}\n",
      "dialog_BERT:  (0.36442309617996216, 0.39747434854507446, 0.38023123145103455)\n"
     ]
    }
   ],
   "source": [
    "print(\"dialog_BLEU: \", dialog_BLEU)\n",
    "print(\"dialog_ROUGE: \", dialog_ROUGE)\n",
    "print(\"dialog_BERT: \", dialog_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dialog_BLEU:  0.6122821503865296\n",
    "dialog_ROUGE:  {'rouge-1': {'r': 0.16851587301587298, 'p': 0.06309809007293622, 'f': 0.08786207706084463}, 'rouge-2': {'r': 0.026555555555555554, 'p': 0.007242070477196334, 'f': 0.010955777178062023}, 'rouge-l': {'r': 0.15362698412698403, 'p': 0.05770404941573766, 'f': 0.0802271028245546}}\n",
    "dialog_BERT:  (tensor([0.8295, 0.8478, 0.8036, 0.8254, 0.8398, 0.8411, 0.8241, 0.8294, 0.8504,\n",
    "        0.8602, 0.8308, 0.8472, 0.8366, 0.8490, 0.8224, 0.8406, 0.8175, 0.8025,\n",
    "        0.8282, 0.8406, 0.8321, 0.8075, 0.8360, 0.8153, 0.8258, 0.8248, 0.8314,\n",
    "        0.8650, 0.7890, 0.8166, 0.8175, 0.7992, 0.8154, 0.8327, 0.8505, 0.8288,\n",
    "        0.8278, 0.8261, 0.8228, 0.8768, 0.8196, 0.7798, 0.8394, 0.8461, 0.8167,\n",
    "        0.8704, 0.8328, 0.8117, 0.8078, 0.8129, 0.7982, 0.8124, 0.8299, 0.8071,\n",
    "        0.9225, 0.8163, 0.7616, 0.8110, 0.8356, 0.8555, 0.8156, 0.8235, 0.8138,\n",
    "        0.8602, 0.8062, 0.8387, 0.8261, 0.8409, 0.8401, 0.8041, 0.8427, 0.8343,\n",
    "        0.8233, 0.8115, 0.8258, 0.8349, 0.8291, 0.8485, 0.8065, 0.8300, 0.8416,\n",
    "        0.8158, 0.7933, 0.8107, 0.8177, 0.8150, 0.8092, 0.8438, 0.8679, 0.8318,\n",
    "        0.8566, 0.8124, 0.8295, 0.7854, 0.7797, 0.8130, 0.8132, 0.8932, 0.8113,\n",
    "        0.8561]), tensor([0.8157, 0.8582, 0.8030, 0.8380, 0.8399, 0.8211, 0.8161, 0.8343, 0.8365,\n",
    "        0.8502, 0.8002, 0.8354, 0.8205, 0.8386, 0.8148, 0.8523, 0.8122, 0.8135,\n",
    "        0.8307, 0.9475, 0.8064, 0.8259, 0.8750, 0.8178, 0.8477, 0.8514, 0.8245,\n",
    "        0.8460, 0.8227, 0.8205, 0.8097, 0.8196, 0.8245, 0.8939, 0.8335, 0.8164,\n",
    "        0.8303, 0.8418, 0.8207, 0.9143, 0.8164, 0.8234, 0.8236, 0.9356, 0.7739,\n",
    "        0.8624, 0.8177, 0.8105, 0.8313, 0.8314, 0.8091, 0.8372, 0.8194, 0.8097,\n",
    "        0.8331, 0.8304, 0.7880, 0.8103, 0.8311, 0.8981, 0.8121, 0.8180, 0.8212,\n",
    "        0.8515, 0.7684, 0.8373, 0.8215, 0.8380, 0.8511, 0.8066, 0.8427, 0.8283,\n",
    "        0.8206, 0.8208, 0.7957, 0.8676, 0.8237, 0.8243, 0.8302, 0.8302, 0.8730,\n",
    "        0.8048, 0.8252, 0.8516, 0.8159, 0.8255, 0.8146, 0.8305, 0.8900, 0.8042,\n",
    "        0.8890, 0.8106, 0.8354, 0.8033, 0.8086, 0.8095, 0.8145, 0.8976, 0.8179,\n",
    "        0.8120]), tensor([0.8225, 0.8530, 0.8033, 0.8317, 0.8398, 0.8310, 0.8201, 0.8318, 0.8434,\n",
    "        0.8552, 0.8152, 0.8412, 0.8285, 0.8438, 0.8186, 0.8464, 0.8149, 0.8080,\n",
    "        0.8295, 0.8909, 0.8190, 0.8166, 0.8550, 0.8166, 0.8366, 0.8379, 0.8279,\n",
    "        0.8554, 0.8055, 0.8185, 0.8136, 0.8093, 0.8199, 0.8622, 0.8419, 0.8226,\n",
    "        0.8290, 0.8339, 0.8217, 0.8952, 0.8180, 0.8010, 0.8314, 0.8886, 0.7947,\n",
    "        0.8664, 0.8252, 0.8111, 0.8194, 0.8220, 0.8036, 0.8246, 0.8246, 0.8084,\n",
    "        0.8755, 0.8233, 0.7746, 0.8106, 0.8333, 0.8762, 0.8138, 0.8207, 0.8175,\n",
    "        0.8558, 0.7869, 0.8380, 0.8237, 0.8394, 0.8456, 0.8054, 0.8427, 0.8313,\n",
    "        0.8220, 0.8161, 0.8105, 0.8509, 0.8264, 0.8362, 0.8182, 0.8301, 0.8570,\n",
    "        0.8102, 0.8089, 0.8307, 0.8168, 0.8202, 0.8119, 0.8371, 0.8788, 0.8177,\n",
    "        0.8725, 0.8115, 0.8324, 0.7943, 0.7939, 0.8112, 0.8138, 0.8954, 0.8146,\n",
    "        0.8334]))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workflowExtract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
