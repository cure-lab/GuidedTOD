{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline should be like:\n",
    "1. Core: could predict next action given the current context\n",
    "2. can load the data from the benchmark\n",
    "3. connect the data to the core module, to get the run-time action prediction list\n",
    "4. Use CE metric to evaluate the quality of the predicted action list\n",
    "5. if good enough, then save the dialogues as the new data for further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/31/2024 22:51:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu:  8istributed hf_training: True 16-bits hf_training: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:314] 2024-03-31 22:51:51,225 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[WARNING|trainer.py:2228] 2024-03-31 22:51:55,977 >> There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4600' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4600/138 : < :, Epoch 100/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =      100.0\n",
      "  total_flos               = 24844709GF\n",
      "  train_loss               =        0.0\n",
      "  train_runtime            = 0:00:00.07\n",
      "  train_samples            =       2895\n",
      "  train_samples_per_second = 117711.309\n",
      "  train_steps_per_second   =    1870.37\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Set the openai GPT-4\n",
    "'''\n",
    "from openai import OpenAI\n",
    "clientGPT4 = OpenAI(api_key=\"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\")\n",
    "clientGPT3_5 = OpenAI(api_key=\"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\")\n",
    "\n",
    "'''\n",
    "Set the AST module for predict the next action: \n",
    "For demo, use the model for SGD dataset \n",
    "'''\n",
    "\"\"\"\n",
    "Reference: https://github.com/huggingface/transformers/tree/main/examples/pytorch\n",
    "\n",
    "Adapted from huggingface Transformers\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "import transformers.trainer_utils as hf_trainer_utils\n",
    "import numpy as np\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    ")\n",
    "\n",
    "from src.data.data_args import DataArguments\n",
    "from src.data.dataset_loader import DatasetLoader\n",
    "from src.data.utils import group_col_name\n",
    "from src.metrics import create_compute_metric_fct, verify_nltk\n",
    "from src.model.hf_model_args import HfModelArguments\n",
    "from src.hf_training.hf_training_args import HfSeq2SeqTrainingArgs\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def train(trainer, train_dataset, training_args):\n",
    "    logger.info(\"*** train ***\")\n",
    "\n",
    "    check_point = get_resume_checkpoint(training_args)\n",
    "    train_result = trainer.train(resume_from_checkpoint=check_point)\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"train_samples\"] = len(train_dataset)\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "\n",
    "def do_eval(trainer, validation_dataset, max_length, num_beams):\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n",
    "\n",
    "    metrics[\"eval_samples\"] = len(validation_dataset)\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "def do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams):\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "        return preds, labels\n",
    "\n",
    "    def decode(preds, labels):\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        if data_args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        if data_args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        model_path = Path(model_args.model_name_or_path)\n",
    "        file_name = \"pred_mwoz.txt\" if training_args.is_mwoz else \"preds_test_set.txt\"\n",
    "        if not model_path.exists():\n",
    "            # model name\n",
    "            preds_file_path = Path(training_args.output_dir) / file_name\n",
    "        else:\n",
    "            preds_file_path = model_path / file_name\n",
    "\n",
    "        with preds_file_path.open(\"w\") as f:\n",
    "            for pred, label in zip(decoded_preds, decoded_labels):\n",
    "                label = label.replace(\"\\n\", \" \")\n",
    "                pred = pred.replace(\"\\n\", \" \")\n",
    "                f.write(f\"{pred}\\t{label}\" + \"\\n\")\n",
    "\n",
    "        return decoded_preds, decoded_labels\n",
    "    logger.info(\"*** Predict ***\")\n",
    "\n",
    "    metrics = {}\n",
    "    predictions = []\n",
    "    if group_col_name in test_dataset.column_names:\n",
    "        group_idx = 0\n",
    "\n",
    "        while True:\n",
    "            group_dataset = test_dataset.filter(lambda x: x[group_col_name] == group_idx)\n",
    "            if group_dataset.num_rows == 0:\n",
    "                # no groups left\n",
    "                break\n",
    "            logger.info(\"Predicting on test group %d\", group_idx)\n",
    "\n",
    "            predict_results = trainer.predict(\n",
    "                group_dataset,\n",
    "                metric_key_prefix=f\"predict_group_{group_idx}\",\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams\n",
    "            )\n",
    "            metrics.update(predict_results.metrics)\n",
    "            metrics[f\"predict_samples_group_{group_idx}_size\"] = len(group_dataset)\n",
    "\n",
    "            group_idx += 1\n",
    "\n",
    "            predictions.append(predict_results.predictions)\n",
    "\n",
    "        for key in [\"loss\", \"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "            metrics[f\"overall_predict_{key}\"] = round(\n",
    "                sum([metrics[f\"predict_group_{idx}_{key}\"] for idx in range(group_idx)]) / group_idx, 4\n",
    "            )\n",
    "    else:\n",
    "        '''\n",
    "        here\n",
    "        '''\n",
    "        # print(\"test_dataset.column_names: \", test_dataset.column_names)\n",
    "        # print(\"test_dataset: \", test_dataset)\n",
    "        # print(\"test_dataset[:2]: \", test_dataset[:2])\n",
    "        # sample_test_dataset = test_dataset.filter(lambda x: x[\"sample_id\"] in [0, 1, 3])\n",
    "        # print(\"sample_test_dataset[\\\"sample_id\\\"]: \", sample_test_dataset[\"sample_id\"])\n",
    "        # sample_test_dataset[\"sample_id\"] = [0, 1, 2]\n",
    "        # print(\"sample_test_dataset[\\\"sample_id\\\"]: \", sample_test_dataset[\"sample_id\"])\n",
    "        # sample_test_dataset[\"input_ids\"] = [sample_test_dataset[\"input_ids\"][0], sample_test_dataset[\"input_ids\"][1], [1,2,3,4,5]]\n",
    "        # print(\"sample_test_dataset[\\\"input_ids\\\"]: \", sample_test_dataset[\"input_ids\"])\n",
    "\n",
    "        # print(\"sample_test_dataset: \", sample_test_dataset)\n",
    "        # print(test_dataset[\"sample_id\"])\n",
    "        # print(test_dataset[\"input_ids\"])\n",
    "        # print(test_dataset[\"labels\"])\n",
    "        \n",
    "        predict_results = trainer.predict(\n",
    "            test_dataset, metric_key_prefix=\"test\", max_length=max_length, num_beams=num_beams\n",
    "        )\n",
    "        # print(\"predict_results: \", predict_results)\n",
    "        # print(\"predict_results.predictions: \", predict_results.predictions)\n",
    "        metrics = predict_results.metrics\n",
    "        metrics[\"predict_samples_size\"] = len(test_dataset)\n",
    "\n",
    "    # trainer.log(metrics)\n",
    "    # trainer.log_metrics(\"test\", metrics)\n",
    "    # trainer.save_metrics(\"test\", metrics)\n",
    "\n",
    "    return decode(predict_results.predictions, test_dataset[\"labels\"])\n",
    "\n",
    "\n",
    "def load_model(model_args, data_args, tokenizer):\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    # Forcing the generation min lenght, to avoid models preset for summarization tasks that are usually high\n",
    "    config.min_length = 5\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    if task_specific_params is not None:\n",
    "        model.config.update(task_specific_params.get(\"summarization_cnn\", {}))\n",
    "\n",
    "    if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n",
    "        if isinstance(tokenizer, MBartTokenizer):\n",
    "            model.config.decoder_start_token_id = tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "        else:\n",
    "            model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(\"en_XX\")\n",
    "\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "    if (\n",
    "        hasattr(model.config, \"max_position_embeddings\")\n",
    "        and model.config.max_position_embeddings < data_args.max_source_length\n",
    "    ):\n",
    "        if model_args.resize_position_embeddings is None:\n",
    "            logger.warning(\n",
    "                \"Increasing the model's number of position embedding vectors from\"\n",
    "                f\" {model.config.max_position_embeddings} to {data_args.max_source_length}.\"\n",
    "            )\n",
    "            model.resize_position_embeddings(data_args.max_source_length)\n",
    "        elif model_args.resize_position_embeddings:\n",
    "            model.resize_position_embeddings(data_args.max_source_length)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`--max_source_length` is set to {data_args.max_source_length}, but the model only has\"\n",
    "                f\" {model.config.max_position_embeddings} position encodings. Consider either reducing\"\n",
    "                f\" `--max_source_length` to {model.config.max_position_embeddings} or to automatically resize the\"\n",
    "                \" model's position encodings by passing `--resize_position_embeddings`.\"\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_resume_checkpoint(training_args):\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    last_checkpoint = get_last_checkpoint(training_args)\n",
    "    if last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def get_last_checkpoint(training_args):\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = hf_trainer_utils.get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming hf_training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "    return last_checkpoint\n",
    "\n",
    "\n",
    "def setup_logging(training_args):\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "def create_data_collector(model, tokenizer, training_args, data_args):\n",
    "    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    return DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "\n",
    "\n",
    "def setup_wandb(training_args):\n",
    "    if training_args.use_wandb:\n",
    "        os.environ[\"WANDB_PROJECT\"] = training_args.wandb_project_name\n",
    "        training_args.run_name = training_args.experiment_name\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = HfArgumentParser((HfModelArguments, DataArguments, HfSeq2SeqTrainingArgs))\n",
    "    model_args, data_args, training_args, _ = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "\n",
    "    name_parts = [training_args.experiment_name]\n",
    "    name_parts.extend([data_args.text_column, data_args.summary_column])\n",
    "\n",
    "    name_parts.append(model_args.model_name_or_path)\n",
    "\n",
    "    training_args.experiment_name = \"_\".join(name_parts)\n",
    "\n",
    "    training_args.output_dir = str(Path(training_args.output_dir).joinpath(training_args.experiment_name))\n",
    "\n",
    "    if data_args.source_prefix is None and model_args.model_name_or_path in [\n",
    "        \"t5-small\",\n",
    "        \"t5-base\",\n",
    "        \"t5-large\",\n",
    "        \"t5-3b\",\n",
    "        \"t5-11b\",\n",
    "    ]:\n",
    "        logger.warning(\n",
    "            \"You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with \"\n",
    "            \"`--source_prefix 'summarize: ' `\"\n",
    "        )\n",
    "    return data_args, model_args, training_args\n",
    "\n",
    "# def hf_run():\n",
    "data_args, model_args, training_args = get_args()\n",
    "\n",
    "setup_wandb(training_args)\n",
    "\n",
    "setup_logging(training_args)\n",
    "\n",
    "verify_nltk()\n",
    "\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: % distributed hf_training: %s 16-bits hf_training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "datasets_loader = DatasetLoader(data_args, training_args, tokenizer)\n",
    "train_dataset, validation_dataset, test_dataset = datasets_loader.load_datasets()\n",
    "\n",
    "model = load_model(model_args, data_args, tokenizer)\n",
    "\n",
    "if training_args.label_smoothing_factor > 0 and not hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "    logger.warning(\n",
    "        \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n",
    "        \"`%s`. This will lead to loss being calculated twice and will take up more memory\",\n",
    "        model.__class__.__name__,\n",
    "    )\n",
    "metric_fct = create_compute_metric_fct(tokenizer, data_args, training_args, model_args)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=create_data_collector(model, tokenizer, training_args, data_args),\n",
    "    compute_metrics=metric_fct if training_args.predict_with_generate else None,\n",
    ")\n",
    "\n",
    "if training_args.do_train:\n",
    "    train(trainer, train_dataset, training_args)\n",
    "\n",
    "max_length = (\n",
    "    training_args.generation_max_length\n",
    "    if training_args.generation_max_length is not None\n",
    "    else data_args.val_max_target_length\n",
    ")\n",
    "num_beams = data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n",
    "\n",
    "# if training_args.do_eval:\n",
    "#     do_eval(trainer, validation_dataset, max_length, num_beams)\n",
    "\n",
    "# if training_args.do_predict:\n",
    "#     results_pred, results_label = do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams)\n",
    "#     # print(\"results_pred: \", results_pred)\n",
    "#     # print(\"results_label: \", results_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only generate the action list, not the full dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_prompt = \"\"\"\n",
    "The following are conversations between a user and an assistant. Indicated by the dialog acts, the assistant can help the user with checking in or providing information of temperature, time, price, location, and so on.\n",
    "The response should be coherent, engaging, diverse, informative, and overall good and should be in line with the next action.\n",
    "The response should be concise and to the point and not exceed 30 words.\n",
    "\n",
    "Available Dialog acts:\n",
    "confirm appointment_date, confirm appointment_time, confirm check_in_date, confirm date, confirm destination, confirm event_name, confirm location, confirm number_of_rooms, confirm number_of_seats, confirm pickup_location, confirm pickup_time, confirm property_name, confirm restaurant_name, confirm time, confirm visit_date, inform address, inform category, inform date, inform free_entry, inform from_station, inform genre, inform good_for_kids, inform humidity, inform inbound_arrival_time, inform outbound_arrival_time, inform phone_number, inform price, inform rating, inform ride_fare, inform time, inform to_station, inform wind, inform_count count, offer address, offer airlines, offer album, offer appointment_date, offer appointment_time, offer artist, offer attraction_name, offer car_name, offer category, offer location, offer date, offer event_name, offer inbound_departure_time, offer movie_name, offer outbound_departure_time, offer pickup_location, offer precipitation, offer price, offer property_name, offer restaurant_name, offer rating, offer temperature, offer time, offer title, offer type, offer_intent intent, request appointment_date, request appointment_time, request area, request category, request check_in_date, request date, request departure_date, request destination, request event_name, request genre, request location, request number_of_beds, request number_of_seats, request pickup_time, request restaurant_name, request return_date, request time, request type, request visit_date, notify_failure, notify_success, req_more, goodbye.\n",
    "\n",
    "Conversation: \n",
    "User: i want to make a restaurant reservation for 2 people at half past 11 in the morning.  \n",
    "Assistant(request restaurant_name [none], request location [none]): what city do you want to dine in? do you have a preferred restaurant? \n",
    "User: please find restaurants in san jose. can you try sino?  \n",
    "Assistant(confirm restaurant_name [sino]. confirm location [san jose]. confirm time [11:30 am]. confirm number_of_seats [2]. confirm date [today]): confirming: i will reserve a table for 2 people at sino in san jose. the reservation time is 11:30 am today. \n",
    "User: yes, thanks. what's their phone number?  \n",
    "Assistant(inform phone_number [408-247-8880], notify_success [none]): your reservation has been made. their phone number is 408-247-8880. \n",
    "User: what's their address? do they have vegetarian options on their menu?  \n",
    "Assistant(inform address [377 santana row #1000]): the street address is 377 santana row #1000. they have good vegetarian options. \n",
    "User: thanks very much.  \n",
    "Assistant(req_more [none]): is there anything else i can help you with? \n",
    "User: no, that's all. thanks.\n",
    "\n",
    "Conversation:\n",
    "User: i'm looking for something like a performing arts venue to visit, can you help me?  \n",
    "Assistant(request location [none]): where would you like me to search? \n",
    "User: search in paris, france please.  \n",
    "Assistant(offer attraction_name [arab world institute], offer category [performing arts venue]): there's the arab world institute, which is a performing arts venue. \n",
    "User: can i bring children there, also is this place free to go to?  \n",
    "Assistant(inform good_for_kids [false], inform free_entry [false]): i don't think children would enjoy this venue, and it is not free. \n",
    "User: okay, cool. that's all i need for now, bye.\n",
    "\n",
    "Conversation:\n",
    "User: hi there, could you find some pop music events for me? \n",
    "Assistant: where are you interested in? \n",
    "User: something in philly is fine.  \n",
    "Assistant(offer event_name [conan gray], offer date [march 1st], offer time [8:30 pm]): would you like conan gray event at the fillmore philadelphia occurring on march 1st at 8:30 pm? \n",
    "User: sure, could you find something else on march 4th in sd instead?  \n",
    "Assistant(offer event_name [tuxedo], offer date [march 4th], offer time [7 pm], inform_count count [1]): there is 1 event called tuxedo at the observatory north park at 7 pm on march 4th. \n",
    "User: sure, that sounds good!  \n",
    "Assistant(offer_intent intent [buyeventtickets]): would you like me to purchase the tickets? \n",
    "User: perhaps later!  \n",
    "Assistant(req_more [none]): could i further assist you? \n",
    "User: could you get me a bus ride there too? \n",
    "Assistant: sure, do you intend to depart on march 4th? \n",
    "User: no, i will leave on march 3rd instead. \n",
    "Assistant: and where will you be travelling from? \n",
    "User: i will travel from lax.  \n",
    "Assistant(inform_count count [10]): there are 10 available buses, would you like one which departs at 7:40 am with 0 transfers costing $28? \n",
    "User: that sounds great! could you get me a roundtrip flight there too?  \n",
    "Assistant(request return_date [none]): sure, when will you be returning? \n",
    "User: i will return next thursday.  \n",
    "Assistant(request departure_date [march 3rd]): and will you depart on march 3rd or march 4th. \n",
    "User: preferably march 3rd, please.  \n",
    "Assistant(offer airlines [alaska airlines], offer outbound_departure_time [6:55 am], offer inbound_departure_time [9:55 am], offer price [$116]): how about an alaska airlines flight departing at 6:55 am and returning at 9:55 am costing $116? \n",
    "User: are there any alternatives?  \n",
    "Assistant(offer airlines [american airlines], offer outbound_departure_time [8:55 pm], offer inbound_departure_time [7:10 pm], offer price [$127]): how about an american airlines flight departing at 8:55 pm and returning at 7:10 pm costing $127? \n",
    "User: which airport will i depart from? \n",
    "Assistant: you will depart from lax international airport. \n",
    "User: sure, could you get me hotels in that area too?  \n",
    "Assistant(offer rating [3]): how about a 3 star hotel called bahia resort hotel? \n",
    "User: are there any alternatives?  \n",
    "Assistant(offer rating [2]): how about a 2 star hotel called best western mission bay? \n",
    "User: that sounds great too!  \n",
    "Assistant(offer_intent intent [reservehotel]): would you like me to make a reservation? \n",
    "User: no thanks, that is not needed.\n",
    "\n",
    "Conversation: \n",
    "[[DIALOG]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def postprocess_predictions(prediction_str):\n",
    "    # print(\"prediction_str: \", prediction_str)\n",
    "    match = re.match(r\"(.*)\\[(.*)]\", prediction_str)\n",
    "    if match:\n",
    "        # action w/ value\n",
    "        action_name = match.group(1).strip()\n",
    "        slot_str = match.group(2)\n",
    "        slot_str = slot_str.replace(\";\", \",\")\n",
    "        slots = [s.strip() for s in slot_str.split(\",\")]\n",
    "        for i in range(len(slots)):\n",
    "            if slots[i].endswith(\">\") and not slots[i].startswith(\"<\"):\n",
    "                # add \"<\" to the beginning of the slot\n",
    "                slots[i] = \"<\" + slots[i]\n",
    "            if slots[i].startswith(\"<\") and not slots[i].endswith(\">\"):\n",
    "                # add \">\" to the end of the slot\n",
    "                slots[i] = slots[i] + \">\"\n",
    "        post_str = action_name + \" \" + \"[\" + \", \".join(slots) + \"]\"\n",
    "        # print(\"post_str: \", post_str)\n",
    "        return post_str\n",
    "    else:\n",
    "        return prediction_str\n",
    "\n",
    "def parse_ast_prediction(prediction_str):\n",
    "    match = re.match(r\"(.*)\\[(.*)]\", prediction_str)\n",
    "    if match:\n",
    "        # action w/ value\n",
    "        action_name = match.group(1).strip()\n",
    "        slot_str = match.group(2)\n",
    "        slot_str = slot_str.replace(\";\", \",\")\n",
    "        slots = [s.strip() for s in slot_str.split(\",\")]\n",
    "        for i in range(len(slots)):\n",
    "            if slots[i].endswith(\">\") and not slots[i].startswith(\"<\"):\n",
    "                # add \"<\" to the beginning of the slot\n",
    "                slots[i] = \"<\" + slots[i]\n",
    "            if slots[i].startswith(\"<\") and not slots[i].endswith(\">\"):\n",
    "                # add \">\" to the end of the slot\n",
    "                slots[i] = slots[i] + \">\"\n",
    "    else:\n",
    "        action_name = \"MISSING\"\n",
    "        slots = [\"MISSING\"]\n",
    "\n",
    "    return action_name, slots\n",
    "\n",
    "def compute_ast_acc_metrics(predictions, labels, convo_ids, turn_ids):\n",
    "    # print(\"predictions: \", predictions)\n",
    "    # print(\"labels: \", labels)\n",
    "    \"\"\"Adapted from ABCD. \"\"\"\n",
    "    action_preds = []\n",
    "    action_labels = []\n",
    "\n",
    "    value_preds = []\n",
    "    value_labels = []\n",
    "\n",
    "    for pred, label in zip(predictions, labels):\n",
    "\n",
    "        action_label, values_label = parse_ast_prediction(label)\n",
    "        values_label.sort()\n",
    "        # for value in values_label:\n",
    "        #     action_labels.append(action_label)\n",
    "        #     value_labels.append(value)\n",
    "        action_labels.append(action_label)\n",
    "        value_labels.append(values_label)\n",
    "\n",
    "        action_pred, values_pred = parse_ast_prediction(pred)\n",
    "        values_pred.sort()\n",
    "\n",
    "        if len(values_pred) > len(values_label):\n",
    "            values_pred = [v for v in values_label if v in values_pred]\n",
    "        if len(values_pred) < len(values_label):\n",
    "            values_pred.extend([\"MISSING\"] * (len(values_label) - len(values_pred)))\n",
    "\n",
    "        # for value in values_pred:\n",
    "        #     action_preds.append(action_pred)\n",
    "        #     value_preds.append(value)\n",
    "        action_preds.append(action_pred)\n",
    "        value_preds.append(values_pred)\n",
    "\n",
    "    # print(\"action_preds: \", action_preds)\n",
    "    # print(\"action_labels: \", action_labels)\n",
    "\n",
    "    # print(\"value_preds: \", value_preds)\n",
    "    # print(\"value_labels: \", value_labels)\n",
    "\n",
    "    action_labels_arrary = np.array(action_labels)\n",
    "    action_preds_arrary = np.array(action_preds)\n",
    "    # print(f\"action_labels_arrary: {action_labels_arrary}\")\n",
    "    # print(f\"action_preds_arrary: {action_preds_arrary}\")\n",
    "    action_match = action_labels_arrary == action_preds_arrary\n",
    "    # print(f\"action_match: {action_match}\")\n",
    "    # print()\n",
    "    action_acc = sum(action_match) / float(len(action_labels))\n",
    "\n",
    "    value_labels_arrary = np.array(value_labels)\n",
    "    value_preds_arrary = np.array(value_preds)\n",
    "    value_match = value_labels_arrary == value_preds_arrary\n",
    "    value_acc = sum(value_match) / float(len(action_labels))\n",
    "\n",
    "    joint_match = action_match & value_match\n",
    "    joint_acc = sum(joint_match) / float(len(action_labels))\n",
    "\n",
    "    # group by convo_ids\n",
    "    unique_convo_ids = list(set(convo_ids))\n",
    "    # print(\"unique_convo_ids: \", unique_convo_ids)\n",
    "    conversations = {}\n",
    "    for uci in unique_convo_ids:\n",
    "        turns, correctness = [], []\n",
    "        correctness_action, correctness_value = [], []\n",
    "        row_id = 0\n",
    "        for convo_id, turn_count in zip(convo_ids, turn_ids):\n",
    "            if convo_id == uci:\n",
    "                turns.append(turn_count)\n",
    "                correct = False\n",
    "                correct_action = False\n",
    "                correct_value = False\n",
    "                action_right = action_match[row_id]\n",
    "                value_right = value_match[row_id]\n",
    "                \n",
    "                if action_right:\n",
    "                    correct_action = True\n",
    "                else:\n",
    "                    correct_action = False\n",
    "                \n",
    "                if value_right:\n",
    "                    correct_value = True\n",
    "                else:\n",
    "                    correct_value = False\n",
    "\n",
    "                if action_right and value_right:\n",
    "                    correct = True\n",
    "                else:\n",
    "                    correct = False\n",
    "\n",
    "                correctness.append(correct)\n",
    "                correctness_action.append(correct_action)\n",
    "                correctness_value.append(correct_value)\n",
    "            row_id += 1\n",
    "\n",
    "        # sort by turn_counts\n",
    "        ordered = [cor for _, cor in sorted(zip(turns, correctness), key=lambda tc: tc[0])]\n",
    "        ordered_action = [cor for _, cor in sorted(zip(turns, correctness_action), key=lambda tc: tc[0])]\n",
    "        ordered_value = [cor for _, cor in sorted(zip(turns, correctness_value), key=lambda tc: tc[0])]\n",
    "        conversations[uci] = [ordered, ordered_action, ordered_value]\n",
    "\n",
    "    # print(\"ordered: \", ordered)\n",
    "    # print(\"ordered_action: \", ordered_action)\n",
    "    # print(\"ordered_value: \", ordered_value)\n",
    "\n",
    "    # count how many correct\n",
    "    turn_score, turn_correct = 0, 0\n",
    "    turn_score_action, turn_correct_action = 0, 0\n",
    "    turn_score_value, turn_correct_value = 0, 0\n",
    "    em_joint, em_action, em_value = [], [], []\n",
    "    my_scores = []\n",
    "    for convo_id, itm in conversations.items():\n",
    "        convo_correctness = itm[0]\n",
    "        convo_correctness_action = itm[1]\n",
    "        convo_correctness_value = itm[2]\n",
    "\n",
    "        # calculate EM\n",
    "        if sum(convo_correctness) == len(convo_correctness):\n",
    "            em_joint.append(True)\n",
    "        else:\n",
    "            em_joint.append(False)\n",
    "        if sum(convo_correctness_action) == len(convo_correctness_action):\n",
    "            em_action.append(True)\n",
    "        else:\n",
    "            em_action.append(False)\n",
    "        if sum(convo_correctness_value) == len(convo_correctness_value):\n",
    "            em_value.append(True)\n",
    "        else:\n",
    "            em_value.append(False)\n",
    "        \n",
    "        # print(f\"convo_id: {convo_id}, convo_correctness: {convo_correctness}\")\n",
    "        current_score = 0\n",
    "        convo_length = len(convo_correctness)\n",
    "        # we use turn_id rather than the true turn_count since turn counts will skip numbers\n",
    "        # when looping through the conversation due to skipping over customer utterances\n",
    "        for turn_id in range(convo_length):\n",
    "            num_remaining = convo_length - turn_id\n",
    "\n",
    "            num_correct = 0\n",
    "            num_correct_action = 0\n",
    "            num_correct_value = 0\n",
    "            # count up how many were predicted correctly\n",
    "            tmp_turn_id = turn_id\n",
    "            while tmp_turn_id < convo_length and convo_correctness[tmp_turn_id]:\n",
    "                num_correct += 1\n",
    "                tmp_turn_id += 1\n",
    "            \n",
    "            tmp_turn_id = turn_id\n",
    "            while tmp_turn_id < convo_length and convo_correctness_action[tmp_turn_id]:\n",
    "                num_correct_action += 1\n",
    "                tmp_turn_id += 1\n",
    "\n",
    "            tmp_turn_id = turn_id\n",
    "            while tmp_turn_id < convo_length and convo_correctness_value[tmp_turn_id]:\n",
    "                num_correct_value += 1\n",
    "                tmp_turn_id += 1\n",
    "\n",
    "            if num_correct > 0:\n",
    "                turn_correct += 1\n",
    "            if num_correct_action > 0:\n",
    "                turn_correct_action += 1\n",
    "            if num_correct_value > 0:\n",
    "                turn_correct_value += 1\n",
    "            # normalize by the number of turns remaining\n",
    "            turn_score += num_correct / num_remaining\n",
    "            turn_score_action += num_correct_action / num_remaining\n",
    "            turn_score_value += num_correct_value / num_remaining\n",
    "            # current_score += num_correct / num_remaining\n",
    "\n",
    "        # my_scores.append(current_score / convo_length)\n",
    "\n",
    "    # normalize by total number of turns possible\n",
    "    '''\n",
    "    len(convo_ids): 200, len(turn_ids): 200\n",
    "    '''\n",
    "    # print(f\"len(convo_ids): {len(convo_ids)}, len(turn_ids): {len(turn_ids)}\")\n",
    "    turn_acc = turn_correct / float(len(convo_ids))\n",
    "    turn_acc_action = turn_correct_action / float(len(convo_ids))\n",
    "    turn_acc_value = turn_correct_value / float(len(convo_ids))\n",
    "    final_score = turn_score / float(len(convo_ids))\n",
    "    final_score_action = turn_score_action / float(len(convo_ids))\n",
    "    final_score_value = turn_score_value / float(len(convo_ids))\n",
    "    \n",
    "    em_action_score = sum(em_action) / float(len(em_action))\n",
    "    em_value_score = sum(em_value) / float(len(em_value))\n",
    "    em_joint_score = sum(em_joint) / float(len(em_joint))\n",
    "\n",
    "    return {\n",
    "        \"EM_action\": round(em_action_score, 4),\n",
    "        \"EM_value\": round(em_value_score, 4),\n",
    "        \"EM_joint\": round(em_joint_score, 4),\n",
    "        \"turn_acc_joint\": round(turn_acc, 4),\n",
    "        \"turn_acc_action\": round(turn_acc_action, 4),\n",
    "        \"turn_acc_value\": round(turn_acc_value, 4),\n",
    "        \"CE_joint\": round(final_score, 4),\n",
    "        \"CE_action\": round(final_score_action, 4),\n",
    "        \"CE_value\": round(final_score_value, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 1 examples [00:00, 34.83 examples/s]\n",
      "Running tokenizer on test dataset: 100%|██████████| 1/1 [00:00<00:00, 21.11 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context:  Context: hello! how may i help you? i just wanted to get some more information about my order. i can't access my account because i forgot my password. okay.may i have your full name and account id? my name is joyce wu and my account id is <pin_number> thank you.\n",
      "agent:  pull-up-account [joyce wu]\n",
      "gold:  pull-up-account [joyce wu]\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 1 examples [00:00, 44.67 examples/s]\n",
      "Running tokenizer on test dataset: 100%|██████████| 1/1 [00:00<00:00, 37.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context:  Context: hello! how may i help you? i just wanted to get some more information about my order. i can't access my account because i forgot my password. okay.may i have your full name and account id? my name is joyce wu and my account id is <pin_number> thank you. joyce what would you like to know? how can i get into my account if i forgot my password? i can help you reset your password. what is your username? my username is <username>\n",
      "agent:  enter-details [<username>]\n",
      "gold:  enter-details [<username>]\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(save_context, w)\n\u001b[1;32m     57\u001b[0m     w\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m train_dataset, validation_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m result_pred, result_label \u001b[38;5;241m=\u001b[39m do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Call the LLM model to generate the response\u001b[39;00m\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/project/llm_framework/AST_abcd_part/src/data/dataset_loader.py:35\u001b[0m, in \u001b[0;36mDatasetLoader.load_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mdo_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mdo_eval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mdo_predict]):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m raw_datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_raw_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m train_dataset, validation_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mdo_train:\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/project/llm_framework/AST_abcd_part/src/data/dataset_loader.py:67\u001b[0m, in \u001b[0;36mDatasetLoader._load_raw_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m     raw_datasets \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_args\u001b[38;5;241m.\u001b[39mdataset_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_args\u001b[38;5;241m.\u001b[39mdataset_config_name, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_args\u001b[38;5;241m.\u001b[39mdata_cache_dir\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     raw_datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_dataset_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_datasets\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/project/llm_framework/AST_abcd_part/src/data/dataset_loader.py:106\u001b[0m, in \u001b[0;36mDatasetLoader._load_dataset_from_files\u001b[0;34m(self, train_file, validation_file, test_file)\u001b[0m\n\u001b[1;32m    101\u001b[0m datasets \u001b[38;5;241m=\u001b[39m load_dataset(extensions[\u001b[38;5;241m0\u001b[39m], data_files\u001b[38;5;241m=\u001b[39mdata_files, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_args\u001b[38;5;241m.\u001b[39mdata_cache_dir)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_file:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# The test dataset contains column not in the train dataset like `group` and the dataset\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# library deletes the columns that does not exits in the train dataset.\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mtest_dataset_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_cache_dir\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mupdate(test_dataset)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/datasets/load.py:2523\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2519\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2520\u001b[0m )\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2523\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/datasets/load.py:2195\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   2194\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 2195\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/datasets/load.py:1730\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m \n\u001b[1;32m   1728\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[0;32m-> 1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/datasets/load.py:1115\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.__init__\u001b[0;34m(self, name, data_dir, data_files, download_config, download_mode)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config \u001b[38;5;241m=\u001b[39m download_config\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_mode \u001b[38;5;241m=\u001b[39m download_mode\n\u001b[0;32m-> 1115\u001b[0m \u001b[43mincrease_load_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/datasets/load.py:283\u001b[0m, in \u001b[0;36mincrease_load_count\u001b[0;34m(name, resource_type)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mHF_DATASETS_OFFLINE \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mHF_UPDATE_DOWNLOAD_COUNTS:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m         \u001b[43mhead_hf_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresource_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/datasets/utils/file_utils.py:98\u001b[0m, in \u001b[0;36mhead_hf_s3\u001b[0;34m(identifier, filename, use_cdn, dataset, max_retries)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhead_hf_s3\u001b[39m(\n\u001b[1;32m     96\u001b[0m     identifier: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, use_cdn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     97\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[requests\u001b[38;5;241m.\u001b[39mResponse, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_head\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_bucket_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cdn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cdn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/datasets/utils/file_utils.py:403\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    401\u001b[0m headers \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(headers) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    402\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_datasets_user_agent(user_agent\u001b[38;5;241m=\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 403\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/datasets/utils/file_utils.py:302\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m tries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectTimeout, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/urllib3/connectionpool.py:776\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m http_tunnel_required \u001b[38;5;129;01mand\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 776\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m, SocketTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    778\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(\n\u001b[1;32m    779\u001b[0m             err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m    780\u001b[0m         )\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/urllib3/connectionpool.py:1045\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     tunnel_scheme \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1039\u001b[0m conn\u001b[38;5;241m.\u001b[39mset_tunnel(\n\u001b[1;32m   1040\u001b[0m     scheme\u001b[38;5;241m=\u001b[39mtunnel_scheme,\n\u001b[1;32m   1041\u001b[0m     host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host,\n\u001b[1;32m   1042\u001b[0m     port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport,\n\u001b[1;32m   1043\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy_headers,\n\u001b[1;32m   1044\u001b[0m )\n\u001b[0;32m-> 1045\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/urllib3/connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_time_off:\n\u001b[1;32m    634\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    635\u001b[0m         (\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem time is way off (before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRECENT_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). This will probably \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[1;32m    640\u001b[0m     )\n\u001b[0;32m--> 642\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39mis_verified\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/urllib3/connection.py:782\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    780\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 782\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/urllib3/util/ssl_.py:470\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/site-packages/urllib3/util/ssl_.py:514\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    511\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/ssl.py:1073\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1071\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1073\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/research/d5/gds/xywen22/anaconda3/envs/workflowExtract/lib/python3.8/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1341\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "\n",
    "def call_LLM(dialogue, Action):\n",
    "\n",
    "    prompt = hint_prompt.replace(\"[[DIALOG]]\", dialogue)\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. You can generate a response to the user's input based on the given previous dialogue and the next action.\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = clientGPT3_5.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.9,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "data = []\n",
    "with open('/research/d5/gds/xywen22/project/llm_framework/AST_abcd_part/data/processed/train_AST_abcd_forDeploy.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_data = json.loads(line)\n",
    "        data.append(json_data)\n",
    "\n",
    "context_list = []\n",
    "distinct_dialogue = {}\n",
    "distinct_dialogue[\"dialogue\"] = []\n",
    "distinct_dialogue[\"pred_action_value\"] = []\n",
    "distinct_dialogue[\"action_value_label\"] = []\n",
    "distinct_dialogue[\"convo_ids\"] = []\n",
    "distinct_dialogue[\"turn_ids\"] = []\n",
    "current_conv_id = 0\n",
    "counter_success_dialogues = 0\n",
    "\n",
    "if os.path.exists(\"data/processed/incremental_data.json\"):\n",
    "    # remove the file\n",
    "    os.remove(\"data/processed/incremental_data.json\")\n",
    "\n",
    "for dialogue_i in range(len(data)):\n",
    "    user_input = data[dialogue_i]['input']\n",
    "    context = user_input\n",
    "    # save the context to a tmp json file:\n",
    "    # {\"sample_id\": 0, \"target\": \"request time [none]\", \"input\": \"Context: hi, could you get me a restaurant booking on the 8th please? \", \"target_data\": \"[\\\"request time\\\", [\\\"none\\\"]]\"}\n",
    "    save_context = {\"sample_id\": data[dialogue_i]['sample_id'], \"convo_id\": data[dialogue_i]['convo_id'], \"turn_id\": data[dialogue_i]['turn_id'], \"target\": data[dialogue_i]['target'], \"input\": context, \"target_data\": data[dialogue_i]['target_data']}\n",
    "    if os.path.exists(\"tmp.json\"):\n",
    "        os.remove(\"tmp.json\")\n",
    "    # print(tmp_sample)\n",
    "    with open(f\"tmp.json\", \"a\") as w:\n",
    "        json.dump(save_context, w)\n",
    "        w.write(\"\\n\")\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = datasets_loader.load_datasets()\n",
    "    result_pred, result_label = do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams)\n",
    "\n",
    "    # Call the LLM model to generate the response\n",
    "    action = result_pred[-1]\n",
    "\n",
    "    action = postprocess_predictions(action)\n",
    "\n",
    "    print(\"context: \", context)\n",
    "    print(\"agent: \", action)\n",
    "    print(\"gold: \", data[dialogue_i]['target'])\n",
    "    print(\"-\" * 30)\n",
    "    print()\n",
    "\n",
    "    if data[dialogue_i]['convo_id'] != current_conv_id:\n",
    "        if current_conv_id == 0:\n",
    "            pass\n",
    "        else:\n",
    "            # calculate the CE metric\n",
    "            metrics = compute_ast_acc_metrics(distinct_dialogue[\"pred_action_value\"], distinct_dialogue[\"action_value_label\"], distinct_dialogue[\"convo_ids\"], distinct_dialogue[\"turn_ids\"])\n",
    "            \n",
    "            # print(\"CE_joint: \", metrics[\"CE_joint\"])\n",
    "            # print(\"CE_action: \", metrics[\"CE_action\"])\n",
    "            # print(\"CE_value: \", metrics[\"CE_value\"])\n",
    "            if metrics[\"CE_joint\"] > 0.5 and metrics[\"CE_action\"] > 0.5 and metrics[\"CE_value\"] > 0.5:\n",
    "                print(\"CE_joint: \", metrics[\"CE_joint\"])\n",
    "                print(\"CE_action: \", metrics[\"CE_action\"])\n",
    "                print(\"CE_value: \", metrics[\"CE_value\"])\n",
    "                print(\"EM action: \", metrics[\"EM_action\"])\n",
    "                print(\"EM value: \", metrics[\"EM_value\"])\n",
    "                print(\"EM joint: \", metrics[\"EM_joint\"])\n",
    "                print(distinct_dialogue[\"pred_action_value\"])\n",
    "                print(distinct_dialogue[\"action_value_label\"])\n",
    "\n",
    "                counter_success_dialogues += 1\n",
    "\n",
    "                for i in range(len(distinct_dialogue[\"dialogue\"])):\n",
    "                    # print(distinct_dialogue[\"dialogue\"][i][\"input\"])\n",
    "                    # print(distinct_dialogue[\"dialogue\"][i][\"predicted_action\"])\n",
    "                    # print(distinct_dialogue[\"dialogue\"][i][\"target\"])\n",
    "                    # print(\"-\" * 30)\n",
    "                    with open(\"data/processed/incremental_data.json\", \"a\") as w:\n",
    "                        json.dump(distinct_dialogue[\"dialogue\"][i], w)\n",
    "                        w.write(\"\\n\")\n",
    "\n",
    "                if counter_success_dialogues == 2:\n",
    "                    break\n",
    "\n",
    "        distinct_dialogue[\"dialogue\"] = []\n",
    "        distinct_dialogue[\"pred_action_value\"] = []\n",
    "        distinct_dialogue[\"action_value_label\"] = []\n",
    "        distinct_dialogue[\"convo_ids\"] = []\n",
    "        distinct_dialogue[\"turn_ids\"] = []\n",
    "\n",
    "        save_context['predicted_action'] = action\n",
    "        distinct_dialogue[\"dialogue\"].append(save_context)\n",
    "        distinct_dialogue[\"pred_action_value\"].append(action)\n",
    "        distinct_dialogue[\"action_value_label\"].append(data[dialogue_i]['target'])\n",
    "        distinct_dialogue[\"convo_ids\"].append(data[dialogue_i]['convo_id'])\n",
    "        distinct_dialogue[\"turn_ids\"].append(data[dialogue_i]['turn_id'])\n",
    "        current_conv_id = data[dialogue_i]['convo_id']\n",
    "    else:\n",
    "        save_context['predicted_action'] = action\n",
    "        distinct_dialogue[\"dialogue\"].append(save_context)\n",
    "        distinct_dialogue[\"pred_action_value\"].append(action)\n",
    "        distinct_dialogue[\"action_value_label\"].append(data[dialogue_i]['target'])\n",
    "        distinct_dialogue[\"convo_ids\"].append(data[dialogue_i]['convo_id'])\n",
    "        distinct_dialogue[\"turn_ids\"].append(data[dialogue_i]['turn_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### can call LLM for generating response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "\n",
    "def call_LLM(dialogue, Action):\n",
    "\n",
    "    prompt = hint_prompt.replace(\"[[DIALOG]]\", dialogue)\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. You can generate a response to the user's input based on the given previous dialogue and the next action.\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = clientGPT3_5.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.9,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "data = []\n",
    "with open('/research/d5/gds/xywen22/project/llm_framework/AST_abcd_part/data/processed/train_AST_abcd.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_data = json.loads(line)\n",
    "        data.append(json_data)\n",
    "\n",
    "context_list = []\n",
    "for dialogue_i in range(len(data)):\n",
    "    if dialogue_i == 10:\n",
    "        break\n",
    "    user_input = data[dialogue_i]['input']\n",
    "    context = user_input\n",
    "    # save the context to a tmp json file:\n",
    "    # {\"sample_id\": 0, \"target\": \"request time [none]\", \"input\": \"Context: hi, could you get me a restaurant booking on the 8th please? \", \"target_data\": \"[\\\"request time\\\", [\\\"none\\\"]]\"}\n",
    "    save_context = {\"sample_id\": 0, \"convo_id\": data[dialogue_i]['convo_id'], \"turn_id\": data[dialogue_i]['turn_id'], \"target\": data[dialogue_i]['target'], \"input\": context, \"target_data\": data[dialogue_i]['target_data']}\n",
    "    if os.path.exists(\"tmp.json\"):\n",
    "        os.remove(\"tmp.json\")\n",
    "    # print(tmp_sample)\n",
    "    with open(f\"tmp.json\", \"a\") as w:\n",
    "        json.dump(save_context, w)\n",
    "        w.write(\"\\n\")\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = datasets_loader.load_datasets()\n",
    "    result_pred, result_label = do_predict(trainer, test_dataset, tokenizer, training_args, data_args, model_args, max_length, num_beams)\n",
    "\n",
    "    # Call the LLM model to generate the response\n",
    "    action = result_pred[-1]\n",
    "\n",
    "    # build the context for the next turn for calling the LLM model\n",
    "    dialog_with_hint = \"\"\n",
    "    for each in context_list:\n",
    "        dialog_with_hint += \"User: \" + each[\"user\"] + \"\\n\" + \"Assistant(\" + each[\"action\"] + \"): \" + each[\"agent\"] + \"\\n\"\n",
    "    dialog_with_hint += \"User: \" + user_input + \"\\n\" + \"Assistant(\" + action + \"): \"\n",
    "\n",
    "    response = call_LLM(dialog_with_hint, action)\n",
    "    print(\"User: \", user_input)\n",
    "    print(\"** Next Action **: \", action)\n",
    "    print(\"Agent: \", response)\n",
    "    context += str(action) + \". \"\n",
    "    if response[-1] not in [\".\", \"?\", \"!\"]:\n",
    "        response += \".\"\n",
    "    context += str(response) + \" \"\n",
    "    context_list.append({\"user\": user_input, \"action\": action, \"agent\": response})\n",
    "    print(\"*\" *100)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"context: \", context)\n",
    "print(\"context_list: \", context_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workflowExtract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
